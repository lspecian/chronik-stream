# v2.2.3 REAL Root Cause - API Misuse in is_leader_ready()

**Date:** 2025-11-08
**Severity:** CRITICAL
**Impact:** Complete cluster mode failure

---

## The REAL Root Cause

The partition leadership conflict is caused by **API misuse** of `RaftCluster::is_leader_ready()`.

### The Bug

**File:** `crates/chronik-server/src/raft_cluster.rs` lines 767-780

```rust
/// Check if Raft has a leader (either we're the leader or there's a valid leader)
///
/// Returns (is_leader_ready, leader_id, state_role)
pub fn is_leader_ready(&self) -> (bool, u64, String) {
    let raft_node = self.raft_node.read().unwrap();
    let state = raft_node.raft.state;
    let leader_id = raft_node.raft.leader_id;

    let is_ready = match state {
        raft::StateRole::Leader => true,        // ✓ This node IS the leader
        raft::StateRole::Follower => leader_id != raft::INVALID_ID,  // ❌ BUG: Returns true for FOLLOWERS too!
        _ => false,
    };

    let state_str = format!("{:?}", state);
    (is_ready, leader_id, state_str)
}
```

**The function documentation says**: "Check if Raft **has** a leader"
**But the function name implies**: "Check if **I am** the leader"
**And the first return value is named**: `is_leader_ready` (ambiguous!)

### How This Breaks leader_election.rs

**File:** `crates/chronik-server/src/leader_election.rs` lines 107-114

```rust
let (is_leader, leader_id, state) = raft_cluster.is_leader_ready();
if !is_leader {
    debug!("Skipping partition leader check - this node is not the Raft leader");
    return;
}
```

**What the code THINKS**:
- `is_leader = true` → "I am the leader, proceed"
- `is_leader = false` → "I am not the leader, skip"

**What ACTUALLY happens**:
- **Node 2 (Leader)**: `is_leader = true` (correct, state=Leader)
- **Node 1 (Follower)**: `is_leader = true` (WRONG! state=Follower but leader_id=2)
- **Node 3 (Follower)**: `is_leader = true` (WRONG! state=Follower but leader_id=2)

**Result**: ALL NODES run the leader election service, not just the Raft leader!

### The Error Chain

1. **All nodes run leader election service** (lines 86-90)
   - Node 1 (follower): `is_leader_ready()` returns `(true, 2, "Follower")`
   - Node 2 (leader): `is_leader_ready()` returns `(true, 2, "Leader")`
   - Node 3 (follower): `is_leader_ready()` returns `(true, 2, "Follower")`

2. **All nodes check partition health** (lines 101-158)
   - All nodes detect timeout for `__meta-0` (leader=1)
   - All nodes call `trigger_election()`

3. **All nodes try to elect new leader** (lines 180-206)
   - All nodes call `elect_leader_from_isr()`
   - All nodes try `raft_cluster.propose_set_partition_leader()`

4. **Proposal fails on followers** (raft_cluster.rs lines 280-286)
   - Node 1: `propose()` checks `state != Leader` → **ERROR**
   - Node 2: `propose()` checks `state == Leader` → ✓ Success
   - Node 3: `propose()` checks `state != Leader` → **ERROR**

5. **Error logged** (leader_election.rs lines 197-205)
   ```
   ❌ Failed to elect leader for __meta-0:
      Cannot propose: this node (id=1) is not the leader (state=Follower, leader=2)
   ```

---

## Why v2.2.1 and v2.2.2 Didn't Fix This

Both releases added the same BROKEN check:

**v2.2.1** (`integrated_server.rs` lines 604-617):
```rust
let (is_ready, leader_id, state) = raft.is_leader_ready();
if is_ready {
    if leader_id == raft.node_id() {  // ← Manual comparison!
        this_node_is_leader = true;
    }
}
```

**v2.2.2** (`produce_handler.rs` lines 2427-2434):
```rust
let (is_leader, leader_id, state) = raft.is_leader_ready();
if !is_leader {  // ← Uses first return value (WRONG!)
    return Ok(());
}
```

**The fixes are INCONSISTENT**:
- `integrated_server.rs`: Manually compares `leader_id == node_id()` (CORRECT)
- `produce_handler.rs`: Uses `is_leader` return value (WRONG - same bug as leader_election.rs!)

---

## The Complete Fix

### Fix 1: Rename and Fix is_leader_ready() ✅

**Make the API clear and unambiguous.**

**Option A: Add a separate function** (recommended)

```rust
/// Check if THIS node is the Raft leader
pub fn am_i_leader(&self) -> bool {
    let raft_node = self.raft_node.read().unwrap();
    raft_node.raft.state == raft::StateRole::Leader
}

/// Check if Raft cluster has a leader (any node)
pub fn is_leader_ready(&self) -> (bool, u64, String) {
    // Keep existing implementation for backward compatibility
    ...
}
```

**Option B: Fix the return value** (breaking change)

```rust
/// Check if Raft has a leader and if THIS node is that leader
///
/// Returns (am_i_leader, leader_id, state_role)
pub fn is_leader_ready(&self) -> (bool, u64, String) {
    let raft_node = self.raft_node.read().unwrap();
    let state = raft_node.raft.state;
    let leader_id = raft_node.raft.leader_id;

    let am_i_leader = state == raft::StateRole::Leader;  // ← FIX: Only true if I'm leader

    let state_str = format!("{:?}", state);
    (am_i_leader, leader_id, state_str)
}
```

### Fix 2: Update All Callers ✅

**Replace all uses of `is_leader_ready()` with clear checks.**

**Files to update:**
- `crates/chronik-server/src/leader_election.rs` line 107
- `crates/chronik-server/src/produce_handler.rs` line 2427
- `crates/chronik-server/src/integrated_server.rs` lines 604-617 (already correct, but make consistent)

**New pattern (using Option A):**
```rust
// OLD (broken):
let (is_leader, leader_id, state) = raft.is_leader_ready();
if !is_leader {
    return;
}

// NEW (correct):
if !raft.am_i_leader() {
    return;
}
```

**Or using Option B:**
```rust
let (am_i_leader, leader_id, state) = raft.is_leader_ready();
if !am_i_leader {
    return;
}
```

### Fix 3: Prefer Raft Leader as Partition Leader ✅

**Same as before - change partition leader assignment to prefer Raft leader.**

Files:
- `crates/chronik-server/src/integrated_server.rs` line 678
- `crates/chronik-server/src/produce_handler.rs` line 2465

```rust
// Set initial leader - prefer Raft leader if it's a replica
let raft_leader_id = raft.node_id();
let leader = if replicas.contains(&raft_leader_id) {
    raft_leader_id
} else {
    replicas[0]
};
```

### Fix 4: Remove Duplicate Code in auto_create_topic() ✅

**Same as before - call `initialize_raft_partitions()` instead of duplicating logic.**

File: `crates/chronik-server/src/produce_handler.rs` lines 2108-2159

```rust
// v2.5.0 Phase 5: Initialize partition assignments in RaftCluster
if self.raft_cluster.is_some() {
    if let Err(e) = self.initialize_raft_partitions(topic_name, metadata.config.partition_count).await {
        warn!("Failed to initialize Raft partitions for '{}': {:?}", topic_name, e);
    }
}
```

---

## Implementation Strategy

### Approach: Option A (Recommended)

**Add new `am_i_leader()` function** and update all callers to use it.

**Pros:**
- ✅ No breaking changes
- ✅ API is now explicit and clear
- ✅ Old function still works for other use cases

**Cons:**
- ❌ Have two similar functions

### Approach: Option B (Clean but Breaking)

**Fix `is_leader_ready()` return value** to mean "am I the leader".

**Pros:**
- ✅ Single function with clear semantics
- ✅ Fixes the root cause directly

**Cons:**
- ❌ Breaking change if anyone else uses this function
- ❌ Need to audit ALL uses

**Recommendation: Use Option A** (add `am_i_leader()`) for safety.

---

## Testing Plan

Same as before, plus:

### Test: Verify Only Raft Leader Runs Elections

```bash
# Start 3-node cluster
docker-compose up -d

# Wait 30 seconds for leader election service to run
sleep 30

# Check which nodes are running partition health checks
docker logs chronik-node-1 | grep "partition leader check"
docker logs chronik-node-2 | grep "partition leader check"
docker logs chronik-node-3 | grep "partition leader check"

# Expected: Only ONE node logs "partition leader check" (the Raft leader)
```

### Test: No "Cannot propose" Errors

```bash
# Check all nodes for proposal errors
docker logs chronik-node-{1,2,3} 2>&1 | grep "Cannot propose"

# Expected: ZERO matches
```

---

## Why This Wasn't Caught Earlier

### Our Testing (v2.2.1)

```bash
# Test duration: 15 seconds
./target/release/chronik-server start --config node{1,2,3}.toml
sleep 15
pkill chronik-server
```

**Why it didn't catch the bug:**
- Leader election service runs every **12 seconds**
- Our test only ran for **15 seconds**
- Might have run once, but errors might not have appeared yet
- **Lesson:** Integration tests need to run for **30+ seconds**

### The Real Issue

The bug is in the **API design** (`is_leader_ready()` return value is ambiguous).

Three different code locations made three different assumptions:
1. `leader_election.rs`: Assumed first return value means "am I leader" ❌
2. `produce_handler.rs`: Assumed first return value means "am I leader" ❌
3. `integrated_server.rs`: Manually compared `leader_id == node_id()` ✅

Only `integrated_server.rs` got it right by not trusting the return value!

---

## Confidence Level

**Very High** - This is the definitive root cause.

**Evidence:**
1. ✅ Explains why ALL nodes try to elect leaders (not just Raft leader)
2. ✅ Explains the exact error message (`this node (id=1) is not the leader`)
3. ✅ Explains why `integrated_server.rs` worked (manual comparison)
4. ✅ Explains why `leader_election.rs` failed (relied on broken return value)
5. ✅ Explains why fresh volumes still fail (not a migration issue)

**Expected Result:** v2.2.3 with these fixes will make cluster mode fully functional.

---

## Version

**Target:** v2.2.3
**Critical:** Yes - cluster mode completely non-functional without this fix
