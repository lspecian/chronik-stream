# P0.2: Move Raft Storage I/O Outside Lock - Implementation Plan

**Task**: P0.2 - Move Raft Storage I/O Outside Lock
**Priority**: P0 - IMMEDIATE (directly fixes P0.5 root cause)
**Effort**: 12 hours estimated
**Risk**: HIGH - Must maintain Raft correctness
**Status**: Planning Complete - Ready for Implementation

---

## Root Cause Analysis (from P0.5 Investigation)

### The Problem

In `raft_cluster.rs` message loop ([raft_cluster.rs:1836-2129](crates/chronik-server/src/raft_cluster.rs#L1836-L2129)):

```rust
// Line 1836: Lock acquired
let mut raft_lock = self.raft_node.lock().await;

// Lines 1839-1857: Process Raft logic (tick, step, ready)
raft_lock.tick();
// ... process messages ...
let mut ready = raft_lock.ready();

// Lines 2047-2063: PERSIST ENTRIES - I/O WHILE HOLDING LOCK ❌
if !ready.entries().is_empty() {
    // Persist while holding lock (tokio::Mutex is async-safe)
    match self.storage.append_entries(&entries_clone).await {
        // DISK I/O HERE - CAN TAKE 1-10ms NORMALLY, 20+ SECONDS DURING LEADER ELECTION
    }
}

// Lines 2067-2082: PERSIST HARD STATE - I/O WHILE HOLDING LOCK ❌
if let Some(hs) = ready.hs() {
    // Persist while holding lock (tokio::Mutex is async-safe)
    match self.storage.persist_hard_state(hs).await {
        // DISK I/O HERE - CAN TAKE 1-10ms NORMALLY, 20+ SECONDS DURING LEADER ELECTION
    }
}

// Line 2109: Advance Raft
raft_lock.advance(ready);
// Lock finally released here
```

### Impact from P0.5 Testing

**Observation**: Topic creation took 25.37 seconds instead of < 1 second.

**Root Cause Chain**:
1. Leader election occurs (Node 1 → Node 2)
2. Raft persists new term + vote while holding `raft_node` lock
3. Disk I/O takes 20+ seconds due to lock contention across all Raft operations
4. `forward_write_to_leader()` retries fail for 20 seconds
5. Topic creation times out (5s timeout) or succeeds after very long delay

**Lock Hold Time**:
- **Normal**: 1-10ms (tick + step + ready + I/O + advance)
- **During Leader Election**: 20+ seconds (I/O blocked by lock contention)

---

## raft-rs Correctness Requirements

From raft-rs documentation and [tikv/raft-rs examples](https://github.com/tikv/raft-rs/blob/master/examples/single_mem_node/main.rs):

**CRITICAL REQUIREMENT**: Lock must be held from `ready()` to `advance()` to prevent:
1. Concurrent modification of Raft state
2. Out-of-order Ready processing
3. Lost messages

**However**, raft-rs does NOT require lock to be held DURING I/O - only that:
1. `ready()` is called once
2. I/O completes before next `ready()`
3. `advance()` is called with the SAME `Ready` instance

---

## Implementation Strategy

### Safe Approach: Persist AFTER Releasing Lock

**Key Insight**: We can release the lock AFTER calling `advance()`, then persist asynchronously.

But wait - raft-rs requires persistence BEFORE sending persisted messages! Let me check the Ready order requirements:

```rust
// Correct order (from raft-rs docs):
1. Send unpersisted messages (no I/O)
2. Persist entries (I/O) - REQUIRED before persisted messages
3. Persist hard state (I/O) - REQUIRED before persisted messages
4. Send persisted messages (no I/O)
5. Advance (marks Ready as processed)
6. Apply committed entries (state machine)
```

So we CANNOT move persistence after advance() - it would break raft-rs's correctness.

### Correct Approach: Clone Data, Release Lock, Persist, Re-Acquire for Advance

**Step-by-Step**:

```rust
// 1. Acquire lock and get Ready
let mut raft_lock = self.raft_node.lock().await;
raft_lock.tick();
// ... step messages ...

if !raft_lock.has_ready() {
    drop(raft_lock);  // Release early if nothing ready
    continue;
}

let mut ready = raft_lock.ready();

// 2. Send unpersisted messages (can do while locked - fast)
if !ready.messages().is_empty() {
    let messages = ready.take_messages();
    // Queue to channel (non-blocking)
    for msg in messages {
        self.message_sender.send((msg.to, msg))?;
    }
}

// 3. Clone data needed for persistence
let entries_to_persist = if !ready.entries().is_empty() {
    Some(ready.entries().iter().cloned().collect::<Vec<_>>())
} else {
    None
};

let hard_state_to_persist = ready.hs().cloned();

// 4. **RELEASE LOCK HERE** - I/O happens outside lock
drop(raft_lock);

// 5. Persist entries (OUTSIDE lock)
if let Some(entries) = entries_to_persist {
    self.storage.append_entries(&entries).await?;
}

// 6. Persist hard state (OUTSIDE lock)
if let Some(hs) = hard_state_to_persist {
    self.storage.persist_hard_state(&hs).await?;
}

// 7. Re-acquire lock BRIEFLY to advance
let mut raft_lock = self.raft_node.lock().await;

// 8. Send persisted messages (fast - queue to channel)
if !ready.persisted_messages().is_empty() {
    let persisted_msgs = ready.take_persisted_messages();
    for msg in persisted_msgs {
        self.message_sender.send((msg.to, msg))?;
    }
}

// 9. Advance Raft (marks Ready as processed)
raft_lock.advance(ready);

// 10. Update cached leader state
let current_leader_id = raft_lock.raft.leader_id;
let current_state = raft_lock.raft.state;
self.cached_leader_id.store(current_leader_id, Ordering::Relaxed);
self.cached_is_leader.store(current_state == raft::StateRole::Leader, Ordering::Relaxed);

// 11. Release lock
drop(raft_lock);

// 12. Apply committed entries (OUTSIDE lock)
if let Some(entries) = committed_entries {
    self.apply_committed_entries(entries).await?;
}
```

**Benefits**:
- Lock held for minimal time (tick + step + ready + advance only)
- I/O happens outside lock (10-100x faster)
- Maintains raft-rs correctness (ready → persist → advance order)

**Risks**:
- Ready object ownership transfer between lock scopes
- Must ensure no `ready()` called between release and re-acquire
- Concurrent messages arriving during I/O (handled by queuing)

---

## Implementation Steps (12 hours)

### Step 1: Refactor Message Loop Structure (4 hours)

**File**: `crates/chronik-server/src/raft_cluster.rs:1807-2200`

**Changes**:
1. Extract persistence logic into separate function:
   ```rust
   async fn persist_ready_state(
       &self,
       entries: Option<Vec<Entry>>,
       hard_state: Option<HardState>,
   ) -> Result<()> {
       if let Some(entries) = entries {
           self.storage.append_entries(&entries).await?;
       }
       if let Some(hs) = hard_state {
           self.storage.persist_hard_state(&hs).await?;
       }
       Ok(())
   }
   ```

2. Modify message loop to clone data before releasing lock
3. Add detailed tracing for lock hold time:
   ```rust
   let lock_start = std::time::Instant::now();
   let mut raft_lock = self.raft_node.lock().await;
   // ... process ...
   let lock_held_ms = lock_start.elapsed().as_millis();
   tracing::debug!("raft_node lock held for {}ms", lock_held_ms);
   ```

**Testing**:
- Unit test: Verify Ready processing order
- Unit test: Verify lock released during I/O
- Integration test: 3-node cluster basic operations

### Step 2: Handle Edge Cases (3 hours)

**Edge Case 1**: Snapshot Application
- Current code already releases lock during snapshot apply (line 1917)
- Verify no regression

**Edge Case 2**: ConfChange Processing
- ConfChange must be applied while holding lock (raft-rs requirement)
- Keep this logic inside lock

**Edge Case 3**: Concurrent Ready Processing
- Add assertion: Only one Ready in flight at a time
- Use atomic flag to prevent concurrent processing

**Testing**:
- Test: Snapshot while messages arriving
- Test: ConfChange during heavy load
- Test: Concurrent ready() calls (should panic)

### Step 3: Add Lock Hold Time Metrics (2 hours)

**File**: `crates/chronik-server/src/raft_cluster.rs`

**Add Metrics**:
```rust
pub struct RaftLockMetrics {
    /// Lock hold time histogram
    lock_hold_time_ms: Arc<RwLock<Vec<u64>>>,

    /// I/O time histogram (when lock is released)
    io_time_ms: Arc<RwLock<Vec<u64>>>,
}

impl RaftCluster {
    fn record_lock_metrics(&self, lock_ms: u64, io_ms: u64) {
        // Record for p50/p95/p99 calculation
    }

    pub fn get_lock_metrics(&self) -> (u64, u64, u64) {
        // Return (p50, p95, p99)
    }
}
```

**Testing**:
- Test: Metrics recorded correctly
- Benchmark: Verify lock hold time < 10ms under load

### Step 4: Comprehensive Testing (2 hours)

**Test Plan**:

1. **Unit Tests**:
   ```rust
   #[tokio::test]
   async fn test_lock_released_during_io() {
       // Verify lock not held during storage.append_entries()
   }

   #[tokio::test]
   async fn test_ready_advance_order() {
       // Verify correct order: ready → persist → advance
   }
   ```

2. **Integration Tests**:
   ```bash
   # Test 1: 3-node cluster with heavy Raft traffic
   ./tests/cluster/start.sh
   # Create 100 topics rapidly (triggers many Raft proposals)
   for i in {1..100}; do
       python3 -c "
       from kafka import KafkaProducer
       producer = KafkaProducer(bootstrap_servers=['localhost:9092'], acks=1)
       producer.send('topic-$i', b'test')
       producer.flush()
       "
   done
   # Verify: No timeouts, topic creation < 5s
   ```

3. **Stress Test**:
   ```bash
   # Test 2: Leader election during heavy load
   ./tests/cluster/start.sh
   # Terminal 1: Heavy produce traffic
   chronik-bench --topic stress --concurrency 64 --duration 60s
   # Terminal 2: Kill leader every 10 seconds
   while true; do
       sleep 10
       kill -9 <leader-pid>
       # Wait for election
       sleep 5
       # Restart node
       ./start_node.sh <node-id>
   done
   # Verify: No 20+ second delays, all operations complete
   ```

**Success Criteria**:
- ✅ Lock hold time p99 < 50ms (vs 20+ seconds before)
- ✅ Topic creation p99 < 5s (vs 25+ seconds before)
- ✅ No deadlocks in 1-hour stress test
- ✅ All Raft correctness tests pass

### Step 5: Documentation & Cleanup (1 hour)

**Documentation**:
1. Update comments in `raft_cluster.rs` explaining lock strategy
2. Add ADR: docs/ADR_RAFT_LOCK_STRATEGY.md
3. Update CLAUDE.md with new lock patterns

**Cleanup**:
1. Remove old "persist while holding lock" comments
2. Add assertions for correctness invariants
3. Remove debug logging added during development

---

## Verification Plan

### Before Implementation (Baseline)

```bash
# Measure current lock hold time
python3 -c "
from kafka import KafkaProducer
import time
topic_name = 'baseline-test'
producer = KafkaProducer(bootstrap_servers=['localhost:9092'], acks=1)
start = time.time()
future = producer.send(topic_name, b'test')
result = future.get(timeout=30)
elapsed = time.time() - start
print(f'Topic creation: {elapsed:.2f}s')
"
# Expected: 25+ seconds during leader election
```

**Baseline Metrics**:
- Lock hold time p99: UNKNOWN (add metrics first)
- Topic creation time: 25+ seconds (from P0.5)
- Timeout rate: ~30% (5s timeout, 25s actual)

### After Implementation (Target)

**Target Metrics**:
- Lock hold time p99: < 50ms (400-500x improvement)
- Topic creation time: < 1 second (25x improvement)
- Timeout rate: < 1%

**Regression Tests**:
```bash
# Test 1: Basic functionality
./tests/cluster/start.sh
python3 -c "
from kafka import KafkaProducer
producer = KafkaProducer(bootstrap_servers=['localhost:9092'], acks=1)
producer.send('test', b'hello')
producer.flush()
"
# Expected: SUCCESS, < 1s

# Test 2: Rapid topic creation
for i in {1..50}; do
    python3 -c "from kafka import KafkaProducer; p = KafkaProducer(bootstrap_servers=['localhost:9092'], acks=1); p.send('topic-$i', b'test'); p.flush()"
done
# Expected: All succeed, p99 < 5s

# Test 3: Leader election stress
# (See Step 4 stress test above)
# Expected: No 20+ second delays
```

---

## Rollback Plan

If implementation causes correctness issues:

1. **Immediate Rollback**:
   ```bash
   git revert <commit-hash>
   cargo build --release
   ./tests/cluster/stop.sh && ./tests/cluster/start.sh
   ```

2. **Diagnostics**:
   - Check Raft logs for out-of-order operations
   - Verify no duplicate Ready processing
   - Check for lost messages

3. **Alternative Approach**:
   - Keep lock during I/O but add timeout (P0.1)
   - Optimize I/O to be faster (smaller batches, better fsync)
   - Add circuit breaker for slow I/O

---

## Success Criteria Summary

**Code Quality**:
- ✅ No new deadlocks introduced
- ✅ All raft-rs correctness requirements met
- ✅ Clean, well-documented code
- ✅ Comprehensive tests (unit + integration)

**Performance**:
- ✅ Lock hold time p99 < 50ms (400-500x improvement)
- ✅ Topic creation p99 < 1 second (25x improvement)
- ✅ No indefinite hangs (all operations timeout)

**Reliability**:
- ✅ No deadlocks in 24-hour stress test
- ✅ Graceful handling of leader elections
- ✅ No data loss or corruption

---

## Related Tasks

**Enabled by P0.2**:
- **P0.3**: Fix v2.2.7 Incomplete Deadlock Fix (verification task)
- **P0.5**: Topic Auto-Creation Timeout (root cause fixed)
- **P1.3**: Complete Event-Driven Metadata Migration (now fast enough)

**Depends on P0.2**:
- None - P0.2 can be implemented independently

---

## Implementation Checklist

### Pre-Implementation
- [x] Read and understand current Raft message loop code
- [x] Document current lock flow and I/O timing
- [x] Identify all locations where lock is held during I/O
- [x] Create this implementation plan
- [ ] Review plan with team (if applicable)
- [ ] Set up test cluster for validation

### Implementation
- [ ] Step 1: Refactor message loop structure (4h)
- [ ] Step 2: Handle edge cases (3h)
- [ ] Step 3: Add lock hold time metrics (2h)
- [ ] Step 4: Comprehensive testing (2h)
- [ ] Step 5: Documentation & cleanup (1h)

### Verification
- [ ] Measure baseline metrics (before fix)
- [ ] Measure post-implementation metrics
- [ ] Run 24-hour stress test
- [ ] Verify no regressions in existing functionality
- [ ] Update IMPLEMENTATION_TRACKER.md

---

**Status**: ✅ READY FOR IMPLEMENTATION
**Next Step**: Begin Step 1 (Refactor message loop structure)
**Estimated Time**: 12 hours total
**Risk Assessment**: HIGH (Raft correctness critical) - Test thoroughly!

