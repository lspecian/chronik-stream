# High Watermark Bug Fix - v2.2.7

## Date: 2025-11-15

## Executive Summary

‚úÖ **CRITICAL BUG FIXED**: `ProduceHandler.get_high_watermark()` was returning the wrong field
üìà **IMPROVEMENT**: Large batch consumption improved from 76.4% to 87.8% success rate
‚ö†Ô∏è **REMAINING**: Still 12.2% messages missing (609/5000), needs further investigation

---

## Bug Discovery

### Symptoms
Large batch consumption (5000+ messages) would stall after consuming ~76-88% of messages:
- **Before fix**: Consumed 3819/5000 (76.4%), then 60s timeout
- **After fix**: Consumed 4391/5000 (87.8%), then 60s timeout
- **Pattern**: Fast initial consumption (7000+ msg/s), then complete stall

### Diagnostic Process

Created debug script (`/tmp/debug_large_batch_consume.py`) that revealed:
```
Consumed 3500 messages in 0.5s at 7253 msg/s
Then stalled for 60 seconds
Consumer waited for high watermark to advance
Timeout after 60s with only 3819/5000 messages
```

This indicated a **watermark synchronization bug** - consumer saw data available initially, then high watermark stopped advancing.

---

## Root Cause Analysis

### PartitionState has TWO offset fields

**File**: [crates/chronik-server/src/produce_handler.rs](../crates/chronik-server/src/produce_handler.rs:250-254)

```rust
struct PartitionState {
    /// Next offset to assign (updated immediately on produce)
    next_offset: AtomicU64,

    /// High watermark (updated based on acks mode and ISR quorum)
    high_watermark: AtomicU64,

    // ... other fields
}
```

**Purpose of each field**:
- `next_offset`: Next offset that will be assigned to incoming produce requests
- `high_watermark`: Highest offset that has been **replicated/acknowledged** and is safe to consume

**Update patterns**:
```rust
// On every produce:
next_offset.store((last_offset + 1) as u64, ...);  // Always immediate

// On produce based on acks mode:
// acks=0: high_watermark.store(last_offset + 1, ...);  // Immediate
// acks=1: high_watermark.store(last_offset + 1, ...);  // Immediate
// acks=-1: high_watermark.store(last_offset + 1, ...); // After ISR quorum
```

### THE BUG

**File**: [crates/chronik-server/src/produce_handler.rs](../crates/chronik-server/src/produce_handler.rs:451-461)

**Before (WRONG)**:
```rust
pub async fn get_high_watermark(&self, topic: &str, partition: i32) -> Result<i64> {
    let key = (topic.to_string(), partition);
    if let Some(state) = self.partition_states.get(&key) {
        let high_watermark = state.value().next_offset.load(Ordering::SeqCst) as i64;  // BUG!
        Ok(high_watermark)
    } else {
        Ok(0)
    }
}
```

**After (CORRECT)**:
```rust
pub async fn get_high_watermark(&self, topic: &str, partition: i32) -> Result<i64> {
    let key = (topic.to_string(), partition);
    if let Some(state) = self.partition_states.get(&key) {
        let high_watermark = state.value().high_watermark.load(Ordering::SeqCst) as i64;  // FIXED!
        Ok(high_watermark)
    } else {
        Ok(0)
    }
}
```

**Impact**: FetchHandler called `get_high_watermark()` to determine what offsets were available for consumption. By returning `next_offset` instead of `high_watermark`, it reported data as available before it was actually replicated/committed.

---

## Why This Caused Stalls

### Cluster Mode with Replication (3 nodes, replication_factor=3)

**Scenario**:
1. Producer sends 5000 messages with `acks=1` (wait for leader acknowledgment)
2. Leader updates `next_offset` immediately to 5000
3. Leader updates `high_watermark` immediately to 5000 (for acks=1)
4. **BUT** in cluster mode with 3-node Raft, there may be delays in watermark propagation

**The Timeline**:
```
Time 0ms:    Producer ‚Üí Node 1 (Leader) ‚Üí Batch 1 (messages 0-999)
Time 10ms:   Leader: next_offset=1000, high_watermark=1000
Time 20ms:   Consumer starts fetching
Time 20ms:   FetchHandler calls get_high_watermark() ‚Üí returns 1000 ‚úì
Time 25ms:   Consumer fetches offsets 0-999 successfully

Time 50ms:   Producer ‚Üí Node 1 ‚Üí Batch 2 (messages 1000-1999)
Time 60ms:   Leader: next_offset=2000, high_watermark=2000
...
Time 500ms:  Leader: next_offset=5000, high_watermark=~4000 (lag starts)
             Consumer sees high_watermark=4000, fetches up to offset 3999

Time 60s:    Consumer timeout - still waiting for high_watermark to advance to 5000
```

**Before the fix**: Consumer saw `next_offset=5000` early, consumed fast, then stalled waiting for data that wasn't actually available.

**After the fix**: Consumer correctly sees `high_watermark=4000`, consumes available data, then stalls waiting for high_watermark to catch up to next_offset.

---

## The Fix

**Commit**: High watermark bug fix v2.2.7
**File**: `crates/chronik-server/src/produce_handler.rs:461`
**Change**: Return `high_watermark` field instead of `next_offset` field

**Diff**:
```diff
- let high_watermark = state.value().next_offset.load(Ordering::SeqCst) as i64;
+ let high_watermark = state.value().high_watermark.load(Ordering::SeqCst) as i64;
```

**Documentation added**:
```rust
/// CRITICAL FIX (v2.2.7): Return actual high_watermark, not next_offset!
/// BUG: Was returning next_offset which is updated immediately on produce
/// CORRECT: Return high_watermark which is updated after ISR acknowledgment
/// This caused large batch consumption to stall - consumer saw next_offset
/// but actual replicated data (high_watermark) lagged behind, causing 60s timeouts.
```

---

## Test Results

### Before Fix
```
Produced:     5000 messages in 11.08s
Consumed:     3819 messages in 60.49s (63 msg/s)
Missing:      1181 messages (23.6%)
Pattern:      Fast to 3500 msgs @ 7253 msg/s, then stall for 60s
```

### After Fix
```
Produced:     5000 messages in 10.92s
Consumed:     4391 messages in 60.50s (73 msg/s)
Missing:      609 messages (12.2%)
Pattern:      Fast to 4000 msgs @ 8077 msg/s, then stall for 60s
Improvement:  +572 messages consumed (48.5% reduction in missing messages)
```

### Improvement Summary
- **Messages consumed**: 3819 ‚Üí 4391 (+572, +15%)
- **Success rate**: 76.4% ‚Üí 87.8% (+11.4 percentage points)
- **Missing messages**: 1181 ‚Üí 609 (-48.5% reduction)

---

## Remaining Issues

### Issue: 12.2% messages still missing (609/5000)

**Likely causes**:
1. **Multi-partition lag**: Topic has 3 partitions, some partitions' watermarks may lag
2. **Watermark sync delay**: Background task syncs watermarks every 5s (may lag under heavy load)
3. **Consumer timeout too short**: 60s may not be enough for watermark to fully catch up
4. **Fetch pagination limits**: Large batches may hit max_bytes limits

**Next steps** (Future work):
1. Add per-partition watermark monitoring
2. Reduce watermark sync interval under heavy load
3. Implement progressive watermark updates
4. Add watermark lag metrics

---

## Impact Assessment

**Positive**:
- ‚úÖ Fixed critical correctness bug (consumer seeing wrong high watermark)
- ‚úÖ Improved large batch consumption success rate by 11.4 percentage points
- ‚úÖ Reduced missing messages by 48.5%
- ‚úÖ No performance regression (consumption rate improved)

**Remaining work**:
- ‚ö†Ô∏è Still 12.2% messages missing in large batches
- ‚ö†Ô∏è Need to investigate watermark lag in cluster mode
- ‚ö†Ô∏è Need better watermark monitoring/metrics

---

## Verification

**Test command**:
```bash
python3 /tmp/debug_large_batch_consume.py
```

**Expected**: ~87.8% success rate (improved from 76.4%)

**Files modified**:
- `crates/chronik-server/src/produce_handler.rs` - Fixed get_high_watermark()

**Related issues**:
- [LOCK_AUDIT_v2.2.7.md](LOCK_AUDIT_v2.2.7.md) - Lock audit completed
- [STABILITY_STATUS_v2.2.7.md](STABILITY_STATUS_v2.2.7.md) - Overall status

---

## Lessons Learned

1. **Don't confuse next_offset with high_watermark** - they serve different purposes
2. **Add tests for high watermark semantics** - this bug wasn't caught by existing tests
3. **Large batch tests are critical** - small batches (1K) didn't expose this bug
4. **Multi-partition scenarios are complex** - watermark synchronization across partitions is non-trivial
5. **Cluster mode adds latency** - replication and Raft consensus introduce delays

---

## Related Documents

- [LOCK_AUDIT_v2.2.7.md](LOCK_AUDIT_v2.2.7.md) - Complete lock usage audit
- [STABILITY_STATUS_v2.2.7.md](STABILITY_STATUS_v2.2.7.md) - v2.2.7 stability status
- [CLUSTER_DEADLOCK_FIX_v2.2.7.md](CLUSTER_DEADLOCK_FIX_v2.2.7.md) - Previous deadlock fixes
- [STABILITY_IMPROVEMENT_PLAN.md](STABILITY_IMPROVEMENT_PLAN.md) - Overall stability plan
