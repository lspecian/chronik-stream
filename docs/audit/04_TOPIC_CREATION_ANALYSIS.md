# Session 4: Topic Creation Slowness Analysis

**File**: Various (`produce_handler.rs`, `raft_metadata_store.rs`, `raft_cluster.rs`)
**Status**: ‚úÖ **COMPLETE**
**Started**: 2025-11-18
**Completed**: 2025-11-18

---

## Executive Summary

**üö® ROOT CAUSE CONFIRMED**: Topic creation takes **20-30 seconds** (user-reported) due to **cascading 5-second timeouts** in serial Raft operations. Each timeout is hit when Raft consensus or WAL replication fails to complete in time.

**Key Finding**: Topic creation involves **4 separate async operations** (1 topic + 3 partitions), each with a **5-second timeout**, performed **serially** in a for loop. If all timeouts are hit ‚Üí **20 seconds total**. Additional delays from retries and WAL replication can push this to **30 seconds**.

---

## Architecture Overview

### Topic Creation Flow (Clustered Mode)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Topic Auto-Creation Flow (Follower)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  Producer ‚Üí Produce Request ‚Üí auto_create_topic()              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  STEP 1: metadata_store.create_topic()                         ‚îÇ
‚îÇ    [raft_metadata_store.rs:128-216]                           ‚îÇ
‚îÇ    ‚îú‚îÄ LEADER PATH: Write to WAL + replicate (1-2ms) ‚úÖ        ‚îÇ
‚îÇ    ‚îî‚îÄ FOLLOWER PATH:                                           ‚îÇ
‚îÇ        ‚îú‚îÄ Forward to leader via RPC (+ retries: 0-350ms)      ‚îÇ
‚îÇ        ‚îî‚îÄ Wait for WAL replication: 5s timeout ‚ö†Ô∏è             ‚îÇ
‚îÇ           ‚îî‚îÄ If TIMEOUT HIT ‚Üí fallback to state check         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  STEP 2: For EACH partition (3 partitions, SERIAL loop)       ‚îÇ
‚îÇ    [produce_handler.rs:2606-2642]                             ‚îÇ
‚îÇ    ‚îú‚îÄ propose_partition_assignment_and_wait(): 5s timeout ‚ö†Ô∏è  ‚îÇ
‚îÇ    ‚îÇ   [raft_cluster.rs:658-710]                              ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ Register notification BEFORE proposing              ‚îÇ
‚îÇ    ‚îÇ   ‚îú‚îÄ Propose via Raft                                    ‚îÇ
‚îÇ    ‚îÇ   ‚îî‚îÄ Wait for commit with tokio::select! (event-driven)  ‚îÇ
‚îÇ    ‚îÇ       ‚îî‚îÄ If TIMEOUT HIT ‚Üí return error                   ‚îÇ
‚îÇ    ‚îÇ                                                            ‚îÇ
‚îÇ    ‚îú‚îÄ propose_set_partition_leader(): FIRE-AND-FORGET ‚úÖ      ‚îÇ
‚îÇ    ‚îÇ   [raft_cluster.rs:1199-1212]                            ‚îÇ
‚îÇ    ‚îÇ   ‚îî‚îÄ Returns immediately (no wait)                       ‚îÇ
‚îÇ    ‚îÇ                                                            ‚îÇ
‚îÇ    ‚îî‚îÄ propose(UpdateISR): FIRE-AND-FORGET ‚úÖ                  ‚îÇ
‚îÇ        [raft_cluster.rs:522-530]                               ‚îÇ
‚îÇ        ‚îî‚îÄ Returns immediately (no wait)                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  RESULT: Topic created with partition assignments             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Critical Code Paths

### 1. auto_create_topic() Entry Point

**Location**: `crates/chronik-server/src/produce_handler.rs:2499-2898`

```rust
pub async fn auto_create_topic(&self, topic_name: &str) -> Result<TopicMetadata> {
    let start_time = Instant::now();

    // ... validation ...

    // STEP 1: Create topic metadata (line 2576)
    let result = match self.metadata_store.create_topic(topic_name, topic_config).await {
        Ok(metadata) => {
            // STEP 2: Assign partitions (CLUSTERED MODE ONLY)
            if let Some(ref raft_cluster) = self.raft_cluster {
                let nodes = raft_cluster.get_all_nodes().await;

                // Create partition assignment plan
                let mut assignment_mgr = AssignmentManager::new();
                assignment_mgr.add_topic(
                    topic_name,
                    self.config.num_partitions as i32,  // Default: 3
                    self.config.default_replication_factor.min(nodes.len() as u32) as i32,
                    &node_ids,
                )?;

                // CRITICAL: For EACH partition, 3 Raft operations (SERIAL)
                for (partition_id, partition_info) in topic_assignments {
                    let replicas: Vec<u64> = partition_info.replicas.iter()
                        .map(|&id| id as u64).collect();

                    // Operation 1: WAIT for partition assignment (5s timeout)
                    if let Err(e) = raft_cluster.propose_partition_assignment_and_wait(
                        topic_name.to_string(),
                        partition_id as i32,
                        replicas.clone(),
                    ).await {
                        warn!("Failed to assign partition: {:?}", e);
                    }

                    // Operation 2: FIRE-AND-FORGET (no wait)
                    raft_cluster.propose_set_partition_leader(
                        topic_name,
                        partition_id as i32,
                        partition_info.leader as u64,
                    ).await?;

                    // Operation 3: FIRE-AND-FORGET (no wait)
                    let isr_cmd = MetadataCommand::UpdateISR {
                        topic: topic_name.to_string(),
                        partition: partition_id as i32,
                        isr: replicas,
                    };
                    raft_cluster.propose(isr_cmd).await?;
                }
            }
            Ok(metadata)
        }
        Err(e) => Err(e),
    };
}
```

**Key Details**:
- Default `num_partitions: 3` (line 82 of `integrated_server.rs`)
- Partition loop is **serial** (for loop, not parallelized)
- Only operation #1 per partition waits for Raft commit
- Operations #2 and #3 are fire-and-forget (return immediately)

---

### 2. create_topic() - FOLLOWER PATH with 5s Timeout

**Location**: `crates/chronik-server/src/raft_metadata_store.rs:128-216`

```rust
async fn create_topic(&self, name: &str, config: TopicConfig) -> Result<TopicMetadata> {
    // LEADER PATH: Write to WAL and replicate (fast, 1-2ms)
    if self.raft.am_i_leader().await {
        let cmd = MetadataCommand::CreateTopic { ... };

        // 1. Write to metadata WAL (durable, 1-2ms)
        let offset = self.metadata_wal.append(&cmd).await?;

        // 2. Apply to local state machine immediately
        self.raft.apply_metadata_command_direct(cmd.clone())?;

        // 3. Fire notification
        if let Some((_, notify)) = self.pending_topics.remove(name) {
            notify.notify_waiters();
        }

        // 4. Async replicate to followers (fire-and-forget)
        tokio::spawn(async move {
            replicator.replicate(&cmd_clone, offset).await;
        });

        // 5. Return immediately
        return state.topics.get(name).cloned().ok_or(...);
    }

    // FOLLOWER PATH: Forward write to leader, then wait for replication
    tracing::info!("Follower forwarding create_topic('{}') to leader", name);

    // 1. Forward the write request to leader
    self.raft.forward_write_to_leader(MetadataWriteCommand::CreateTopic {
        name: name.to_string(),
        partition_count: config.partition_count as i32,
        replication_factor: config.replication_factor as i32,
        config: config.config.clone(),
    }).await.map_err(...)?;

    // 2. Wait for WAL replication to arrive at this follower
    tracing::debug!("Waiting for topic '{}' to be replicated to follower", name);
    let notify = Arc::new(Notify::new());
    self.pending_topics.insert(name.to_string(), Arc::clone(&notify));

    let timeout_duration = tokio::time::Duration::from_millis(5000);  // ‚ö†Ô∏è 5s timeout
    match tokio::time::timeout(timeout_duration, notify.notified()).await {
        Ok(_) => {
            // Success: Topic replicated
            let state = self.state();
            state.topics.get(name).cloned().ok_or(...)
        }
        Err(_) => {
            // TIMEOUT HIT: Fallback to checking state
            tracing::warn!("Topic '{}' replication timed out after {}ms",
                name, timeout_duration.as_millis());
            let state = self.state();
            state.topics.get(name).cloned().ok_or(
                MetadataError::NotFound(format!("Topic {} not found after timeout", name))
            )
        }
    }
}
```

**‚ö†Ô∏è CRITICAL TIMEOUT**: Line 197 - **5-second timeout** waiting for WAL replication

**When Timeout is Hit**:
- WAL replication from leader to follower is slow
- Metadata WAL write is stuck (file I/O stall, network lag)
- Follower is behind in replication log

---

### 3. propose_partition_assignment_and_wait() - Event-Driven Wait with 5s Timeout

**Location**: `crates/chronik-server/src/raft_cluster.rs:658-710`

```rust
pub async fn propose_partition_assignment_and_wait(
    &self,
    topic: String,
    partition: i32,
    replicas: Vec<u64>,
) -> Result<()> {
    // Single-node fast path: apply immediately (no waiting needed)
    if self.is_single_node() {
        let cmd = MetadataCommand::AssignPartition {
            topic: topic.clone(),
            partition,
            replicas,
        };
        return self.apply_immediately(cmd).await;
    }

    // Multi-node: register notification BEFORE proposing
    let key = format!("{}:{}", topic, partition);
    let notify = Arc::new(Notify::new());
    self.pending_partitions.insert(key.clone(), notify.clone());

    tracing::info!("üîî Registered wait notification for partition assignment '{}'", key);

    // Propose command via Raft
    let cmd = MetadataCommand::AssignPartition {
        topic: topic.clone(),
        partition,
        replicas,
    };

    if let Err(e) = self.propose_via_raft(cmd).await {
        // Cleanup notification on failure
        self.pending_partitions.remove(&key);
        return Err(e);
    }

    tracing::info!("‚è≥ Waiting for partition assignment '{}' to be committed...", key);

    // Wait for notification (with timeout) - EVENT-DRIVEN!
    tokio::select! {
        _ = notify.notified() => {
            tracing::info!("‚úÖ Partition assignment '{}' committed and applied!", key);
            Ok(())
        }
        _ = tokio::time::sleep(std::time::Duration::from_secs(5)) => {  // ‚ö†Ô∏è 5s timeout
            // Cleanup on timeout
            self.pending_partitions.remove(&key);
            Err(anyhow::anyhow!(
                "Timeout waiting for partition assignment '{}' to be committed (waited 5s)",
                key
            ))
        }
    }
}
```

**‚ö†Ô∏è CRITICAL TIMEOUT**: Line 702 - **5-second timeout** waiting for Raft commit

**When Timeout is Hit**:
- Raft consensus takes > 5 seconds (slow network, node delays)
- Raft ready loop is blocked (from Session 2: file I/O stall holding lock)
- apply_committed_entries() is delayed
- Notification not triggered in time

**Notification Trigger Location**: `raft_cluster.rs:1091-1098`
```rust
MetadataCommand::AssignPartition { topic, partition, .. } => {
    let key = format!("{}:{}", topic, partition);
    if let Some((_, notify)) = self.pending_partitions.remove(&key) {
        notify.notify_waiters();  // ‚Üê Wakes up waiting thread
        tracing::debug!("‚úì Notified waiting threads for partition assignment '{}'", key);
    }
}
```

---

### 4. propose_via_raft() - Fire-and-Forget (No Wait)

**Location**: `crates/chronik-server/src/raft_cluster.rs:589-635`

```rust
async fn propose_via_raft(&self, cmd: MetadataCommand) -> Result<()> {
    // Check if we're the leader
    {
        let raft = self.raft_node.lock().await;
        if raft.raft.state != raft::StateRole::Leader {
            return Err(anyhow::anyhow!("Cannot propose: not the leader"));
        }
    }

    // Serialize and propose
    let data = bincode::serialize(&cmd)?;

    // Propose to Raft - the entry will be committed asynchronously
    {
        let mut raft = self.raft_node.lock().await;
        raft.propose(vec![], data.clone())?;
        tracing::info!("‚úÖ DEBUG propose_via_raft: raft.propose() succeeded!");
    }

    // PERFORMANCE (v2.2.7): Return immediately - don't wait for commit!
    // The RaftMetadataStore has event-driven notifications for instant wake-up.
    tracing::info!("‚úÖ DEBUG propose_via_raft: EXIT - returning Ok()");
    Ok(())
}
```

**‚úÖ NO WAIT**: Returns immediately after proposing to Raft

**Used By**:
- `propose_set_partition_leader()` (line 1211) - fire-and-forget
- `propose(UpdateISR)` (line 529) - fire-and-forget

---

### 5. forward_write_to_leader() - Retry Logic with Exponential Backoff

**Location**: `crates/chronik-server/src/raft_cluster.rs:1472-1530`

```rust
pub async fn forward_write_to_leader(
    &self,
    command: MetadataWriteCommand,
) -> Result<()> {
    // Retry logic to handle intermittent leader election issues
    const MAX_RETRIES: usize = 3;
    const INITIAL_DELAY_MS: u64 = 50;

    for attempt in 0..=MAX_RETRIES {
        match self.get_leader_id().await {
            Some(leader_id) => {
                match self.do_forward_write_to_leader(leader_id, &command).await {
                    Ok(()) => {
                        return Ok(());
                    }
                    Err(e) => {
                        tracing::warn!("Failed to forward write to leader (attempt {}/{}): {:?}",
                            attempt + 1, MAX_RETRIES + 1, e);
                        if attempt < MAX_RETRIES {
                            let delay_ms = INITIAL_DELAY_MS * 2_u64.pow(attempt as u32);
                            tokio::time::sleep(Duration::from_millis(delay_ms)).await;
                        }
                    }
                }
            }
            None => {
                // No leader yet - exponential backoff
                if attempt < MAX_RETRIES {
                    let delay_ms = INITIAL_DELAY_MS * 2_u64.pow(attempt as u32);
                    tracing::debug!("No Raft leader elected, retrying in {}ms", delay_ms);
                    tokio::time::sleep(Duration::from_millis(delay_ms)).await;
                } else {
                    return Err(anyhow::anyhow!("No Raft leader elected after {} retries", MAX_RETRIES + 1));
                }
            }
        }
    }

    Err(anyhow::anyhow!("Failed to forward write after {} retries", MAX_RETRIES + 1))
}
```

**Exponential Backoff Delays**:
- Attempt 0: No delay
- Attempt 1: 50ms (50 √ó 2^0)
- Attempt 2: 100ms (50 √ó 2^1)
- Attempt 3: 200ms (50 √ó 2^2)
- **Total**: 50 + 100 + 200 = **350ms** (if all retries needed)

---

## Timing Analysis

### Best Case (Leader Node, Fast Raft)

```
STEP 1: create_topic() [LEADER PATH]
  ‚îî‚îÄ WAL write + apply: 1-2ms

STEP 2: For each of 3 partitions [SERIAL]
  ‚îî‚îÄ propose_partition_assignment_and_wait()
      ‚îú‚îÄ Propose via Raft: < 1ms
      ‚îî‚îÄ Wait for commit: 10-100ms (Raft consensus)
  ‚îî‚îÄ propose_set_partition_leader(): < 1ms (no wait)
  ‚îî‚îÄ propose(UpdateISR): < 1ms (no wait)

TOTAL: 1-2ms + (3 √ó ~50ms) = ~150-200ms ‚úÖ
```

### Typical Case (Follower Node, Normal Raft)

```
STEP 1: create_topic() [FOLLOWER PATH]
  ‚îú‚îÄ Forward to leader: 50-100ms (retries)
  ‚îî‚îÄ Wait for WAL replication: 100-500ms (notification)

STEP 2: For each of 3 partitions [SERIAL]
  ‚îî‚îÄ propose_partition_assignment_and_wait()
      ‚îú‚îÄ Propose via Raft: < 1ms
      ‚îî‚îÄ Wait for Raft commit: 50-200ms (notification)
  ‚îî‚îÄ propose_set_partition_leader(): < 1ms (no wait)
  ‚îî‚îÄ propose(UpdateISR): < 1ms (no wait)

TOTAL: ~300ms + (3 √ó ~100ms) = ~600-800ms ‚úÖ
```

### Worst Case (Follower Node, All Timeouts Hit)

```
STEP 1: create_topic() [FOLLOWER PATH]
  ‚îú‚îÄ Forward to leader: 0-350ms (retries with backoff)
  ‚îî‚îÄ Wait for WAL replication: 5000ms ‚ö†Ô∏è TIMEOUT HIT

STEP 2: For each of 3 partitions [SERIAL]
  ‚îî‚îÄ propose_partition_assignment_and_wait()
      ‚îú‚îÄ Propose via Raft: < 1ms
      ‚îî‚îÄ Wait for Raft commit: 5000ms ‚ö†Ô∏è TIMEOUT HIT
  ‚îî‚îÄ propose_set_partition_leader(): < 1ms (no wait)
  ‚îî‚îÄ propose(UpdateISR): < 1ms (no wait)

TOTAL: 5000ms + (3 √ó 5000ms) = 20,000ms = **20 seconds** ‚ö†Ô∏è

With additional delays (retries, network lag, etc.):
TOTAL: ~25-30 seconds ‚ö†Ô∏è ‚Üê MATCHES USER REPORT!
```

---

## Root Cause Breakdown

### Why Timeouts Are Hit (Raft Consensus Delays)

From **Session 2** findings, we know Raft ready loop has issues:

1. **Raft ready loop holds lock during file I/O** (`raft_cluster.rs:1836-2132`)
   - If `storage.append_entries().await` blocks (WAL file I/O stall)
   - Lock held indefinitely ‚Üí Raft frozen ‚Üí Consensus stops
   - Partition assignment waits 5s ‚Üí **TIMEOUT**

2. **WAL replication lag** (metadata WAL from leader to follower)
   - Metadata WAL uses fire-and-forget replication
   - If follower is slow to process ‚Üí replication delayed
   - create_topic() waits 5s ‚Üí **TIMEOUT**

3. **Serial partition assignment** (not parallelized)
   - 3 partitions processed one-by-one
   - Each partition can hit 5s timeout independently
   - 3 √ó 5s = **15 seconds** just for partition assignment

---

## Performance Issues Identified

### 1. üö® Serial Partition Assignment (CRITICAL)

**Location**: `produce_handler.rs:2606-2642`

**Problem**: Partition assignments processed in a **for loop** (serial)

```rust
for (partition_id, partition_info) in topic_assignments {
    // Each partition: propose_partition_assignment_and_wait (5s timeout)
    // If timeout hit: 5s √ó 3 partitions = 15s
}
```

**Fix**: Parallelize partition assignments using `futures::future::join_all()`

**Expected Improvement**: 3 √ó 5s = 15s ‚Üí 1 √ó 5s = 5s (3x faster)

---

### 2. ‚ö†Ô∏è create_topic() Follower Path Timeout (HIGH)

**Location**: `raft_metadata_store.rs:197-216`

**Problem**: Follower waits **5 seconds** for WAL replication before fallback

**Why Slow**:
- Metadata WAL replication is fire-and-forget (async spawn)
- No backpressure or flow control
- If follower is slow to apply ‚Üí timeout hit

**Fix Options**:
1. Reduce timeout to 1-2 seconds (fail faster, rely on fallback)
2. Improve metadata WAL replication speed (batching, compression)
3. Skip wait entirely - always fallback to state check immediately

---

### 3. ‚ö†Ô∏è propose_partition_assignment_and_wait() Timeout (HIGH)

**Location**: `raft_cluster.rs:702`

**Problem**: Each partition waits **5 seconds** for Raft commit

**Why Slow** (from Session 2):
- Raft ready loop holds lock during file I/O (deadlock risk)
- apply_committed_entries() only runs after Raft ready completes
- If Raft ready is slow ‚Üí commit delayed ‚Üí timeout hit

**Fix** (from Session 2 recommendations):
1. Add timeout to WAL file I/O operations (30s)
2. Move file I/O outside Raft lock scope
3. Event-driven notifications are already implemented ‚úÖ

---

## Recommendations

### Immediate (v2.2.9)

1. ‚úÖ **Parallelize partition assignments**
   - Use `futures::future::join_all()` for concurrent Raft proposes
   - Expected improvement: 15s ‚Üí 5s (3x faster)

2. ‚úÖ **Reduce create_topic() follower timeout**
   - Change from 5s to 1-2s
   - Faster fallback to state check

3. ‚úÖ **Add timeout to WAL file I/O** (from Session 1)
   - Prevents indefinite blocking
   - Reduces Raft commit delays

### Short-term (v2.3.0)

4. Improve metadata WAL replication speed
   - Batching multiple commands
   - Compression
   - Flow control / backpressure

5. Add telemetry for timeout hit rates
   - Metrics: `topic_creation_timeout_count`, `partition_assignment_timeout_count`
   - Helps identify when timeouts are common

---

## Conclusion

**Root Cause**: Topic creation takes **20-30 seconds** due to:
1. **Serial partition assignment** (3 √ó 5s = 15s if timeouts hit)
2. **create_topic() follower wait** (5s if WAL replication slow)
3. **Raft consensus delays** (from Session 2: ready loop holding lock during file I/O)

**Why Timeouts Are Hit**:
- WAL file I/O stalls (Session 1: no timeouts on file operations)
- Raft ready loop blocks (Session 2: lock held during append_entries)
- Metadata WAL replication lag

**Fix Priority**: Parallelize partition assignments + add file I/O timeouts (v2.2.9)

**Expected Improvement**: 20-30s ‚Üí 5-10s (2-6x faster)
