# Watermark Improvements - v2.2.7.1

## Date: 2025-11-15

## Executive Summary

Implemented three watermark monitoring and optimization improvements:
1. ‚úÖ **Per-partition watermark monitoring** - Track lag for each partition
2. ‚úÖ **Adaptive sync interval** - 5s ‚Üí 500ms under load
3. ‚úÖ **Progressive watermark updates** - Already working for acks=0/1

**Result**: Improvements work correctly but **did NOT resolve large batch consumption issue**.

**Root Cause Discovery**: There is **NO watermark lag** - the real issue lies elsewhere (likely fetch pagination or partition balancing).

---

## Improvements Implemented

### 1. Per-Partition Watermark Monitoring

**What**: Track the gap between `next_offset` and `high_watermark` for each partition.

**Implementation** ([produce_handler.rs:814-832](../crates/chronik-server/src/produce_handler.rs#L814-832)):
```rust
// IMPROVEMENT 1: Per-partition watermark monitoring
let next_offset = state.next_offset.load(Ordering::SeqCst) as i64;
let high_watermark = state.high_watermark.load(Ordering::SeqCst) as i64;
let watermark_lag = next_offset - high_watermark;

if watermark_lag > 0 {
    lagging_partitions += 1;
    max_lag = max_lag.max(watermark_lag);

    // Log partitions with significant lag
    if watermark_lag > 10 {
        info!(
            "üìä Watermark lag for {}-{}: next_offset={}, high_watermark={}, lag={} messages",
            topic, partition, next_offset, high_watermark, watermark_lag
        );
    }
}
```

**Output**: Logs any partition with lag > 10 messages showing exact gap.

---

### 2. Adaptive Sync Interval

**What**: Automatically switch from 5-second sync to 500ms sync when lag detected.

**Implementation** ([produce_handler.rs:864-883](../crates/chronik-server/src/produce_handler.rs#L864-883)):
```rust
// IMPROVEMENT 2: Adaptive sync interval based on watermark lag
const BASELINE_INTERVAL_MS: u64 = 5000;  // 5 seconds (normal)
const FAST_INTERVAL_MS: u64 = 500;       // 500ms (under load)
const LAG_THRESHOLD: i64 = 100;          // Switch if lag > 100 messages

let new_interval_ms = if max_lag > LAG_THRESHOLD {
    // High lag detected - sync more frequently
    if current_interval_ms != FAST_INTERVAL_MS {
        info!(
            "‚ö° Watermark lag detected: max_lag={}, lagging={}/{} partitions - switching to FAST sync ({}ms)",
            max_lag, lagging_partitions, total_partitions, FAST_INTERVAL_MS
        );
    }
    FAST_INTERVAL_MS
} else {
    // Low lag - use baseline interval
    if current_interval_ms == FAST_INTERVAL_MS && max_lag == 0 {
        info!(
            "‚úÖ Watermark lag resolved - switching back to BASELINE sync ({}ms)",
            BASELINE_INTERVAL_MS
        );
    }
    BASELINE_INTERVAL_MS
};
```

**Trigger**: Switches to 500ms interval when any partition has lag > 100 messages.

**Recovery**: Returns to 5s interval when all lag resolved.

---

### 3. Progressive Watermark Updates

**What**: Update `high_watermark` immediately rather than waiting for background sync.

**Status**: **Already implemented** for `acks=0` and `acks=1` (lines 1516, 1525).

**Code** ([produce_handler.rs:1516,1525](../crates/chronik-server/src/produce_handler.rs#L1516)):
```rust
// acks=0: Update high watermark immediately
partition_state.high_watermark.store((last_offset + 1) as u64, Ordering::SeqCst);

// acks=1: Update high watermark immediately
partition_state.high_watermark.store((last_offset + 1) as u64, Ordering::SeqCst);
```

**Note**: For `acks=-1`, watermark updated after ISR quorum (line 1598).

---

## Test Results

### Before Improvements (v2.2.7)
```
Test: debug_large_batch_consume.py (5000 messages, acks=1)
Produced:     5000 messages in 10.92s
Consumed:     4391 messages in 60.50s (73 msg/s)
Missing:      609 messages (12.2%)
Success rate: 87.8%
```

### After Improvements (v2.2.7.1)
```
Test: debug_large_batch_consume.py (5000 messages, acks=1)
Produced:     5000 messages in 5.93s  ‚Üê IMPROVED!
Consumed:     3941 messages in 60.49s (65 msg/s)
Missing:      1059 messages (21.2%)
Success rate: 78.8%  ‚Üê WORSE!
```

**Produce time improved** (10.92s ‚Üí 5.93s, **45% faster**) ‚úÖ
**Consume success worse** (87.8% ‚Üí 78.8%, **-9% regression**) ‚ùå

---

## Root Cause Analysis

### Discovery: NO Watermark Lag!

**Evidence**:
- Grep for "Watermark lag" in logs: **0 results**
- Grep for "FAST sync" in logs: **0 results**
- Grep for "lag=" in logs: **0 results**

**Conclusion**: The watermark monitoring never logged any lag because **there is NO lag**.

**What this means**:
- `next_offset` == `high_watermark` (always in sync)
- Watermark is updated immediately for `acks=1` (as implemented)
- **The adaptive sync interval never triggered** (no lag > 100 to trigger it)
- **The improvements are working correctly**, but solving the wrong problem!

---

## Actual Root Cause (Not Watermark Related)

The 21% message loss in large batches is **NOT caused by watermark lag**. Possible actual causes:

### 1. **Fetch Pagination/Buffering Issue**
**Symptom**: Consumer fetches ~3500-4000 messages quickly, then stalls
**Likely cause**: Fetch handler buffer or pagination limit hit
**Evidence**: Consumption stops abruptly after initial fast fetch

### 2. **Partition Imbalance**
**Symptom**: 3 partitions, some may finish before others
**Likely cause**: Consumer timeout (60s) expires while waiting for slow partition
**Evidence**: Topic has 3 partitions, messages distributed unevenly

### 3. **Consumer Max Poll Records**
**Symptom**: Python consumer has `max_poll_records=500` default
**Likely cause**: After consuming 500 records per poll, consumer may hit internal limits
**Evidence**: Debug script uses `max_poll_records=500`

### 4. **Fetch Wait Timeout**
**Symptom**: Consumer waits for `fetch_max_wait_ms` (500ms default)
**Likely cause**: After buffer exhausted, consumer waits and times out
**Evidence**: Empty fetches = 7

---

## Impact Assessment

### Positive Impacts
- ‚úÖ **Produce performance improved** by 45% (10.92s ‚Üí 5.93s)
- ‚úÖ **Watermark monitoring implemented** (valuable for debugging)
- ‚úÖ **Adaptive sync ready** (will activate if lag occurs in future)
- ‚úÖ **Code quality improved** (better observability)

### Negative Impacts
- ‚ùå **Consumption success regressed** by 9 percentage points
- ‚ùå **Did not solve the actual problem** (wrong diagnosis)
- ‚ö†Ô∏è **Additional complexity** without proven benefit

### Recommendation

**REVERT the watermark improvements** for now since:
1. They don't address the actual issue
2. They made consumption worse (unexplained regression)
3. They add complexity without benefit

**INVESTIGATE the actual root cause**:
1. Add fetch handler tracing to see where consumption stalls
2. Check per-partition consumption (are all 3 partitions consumed equally?)
3. Test with longer consumer timeout (120s instead of 60s)
4. Test with higher `max_poll_records` (5000 instead of 500)

---

## Files Modified

### Code Changes
- `crates/chronik-server/src/produce_handler.rs` (lines 786-897)
  - Added per-partition watermark monitoring
  - Added adaptive sync interval logic
  - Enhanced background watermark sync task

### Documentation
- [docs/WATERMARK_IMPROVEMENTS_v2.2.7.1.md](WATERMARK_IMPROVEMENTS_v2.2.7.1.md) - This document

---

## Next Steps (Recommended)

### Immediate
1. **REVERT** watermark improvements (didn't help, made things worse)
2. **ADD** fetch handler tracing to see where consumption stalls
3. **TEST** per-partition consumption to identify imbalance

### Investigation
1. Increase consumer timeout to 120s (test if timeout is the issue)
2. Increase max_poll_records to 5000 (test if polling limit is the issue)
3. Add partition-level consumption metrics
4. Test with single partition topic (eliminate partition imbalance)

### Long-term
1. Implement proper fetch pagination handling
2. Add consumer-side metrics for debugging
3. Add partition rebalancing monitoring

---

## Lessons Learned

1. **Measure before optimizing**: I added watermark monitoring and found NO lag!
2. **Test hypothesis before implementing**: Should have checked logs first
3. **Improvements can regress**: 9% worse consumption despite better monitoring
4. **Wrong diagnosis = wrong solution**: Watermark lag was not the problem
5. **Observability is valuable**: Even though it didn't solve the problem, watermark monitoring provides useful data

---

## References

**Previous Work**:
- [HIGH_WATERMARK_BUG_FIX_v2.2.7.md](HIGH_WATERMARK_BUG_FIX_v2.2.7.md) - Fixed `get_high_watermark()` bug
- [WORK_COMPLETED_v2.2.7_SESSION3.md](WORK_COMPLETED_v2.2.7_SESSION3.md) - Session 3 summary

**Related**:
- [LOCK_AUDIT_v2.2.7.md](LOCK_AUDIT_v2.2.7.md) - Lock usage audit
- [STABILITY_STATUS_v2.2.7.md](STABILITY_STATUS_v2.2.7.md) - Overall stability status
