# Production Readiness Analysis - Chronik Raft Cluster

**Date:** 2025-11-01
**Branch:** feat/v2.5.0-kafka-cluster
**Question:** Is this production-ready? What's missing?

## Executive Summary

**Status:** üî¥ **BROKEN - CRASHES ON STARTUP** - Multi-node Raft cluster has critical runtime bug.

**Does it work?** ‚ùå **NO** - Tested on 2025-11-01, crashes immediately with Raft panic.

**Standalone Mode:** ‚úÖ **WORKS PERFECTLY** - Single-node deployment is production-ready.

---

## What's Actually Implemented ‚úÖ

### 1. **Raft Metadata Storage** (JUST IMPLEMENTED)
- ‚úÖ RaftWalStorage - WAL-backed persistent storage for Raft log
- ‚úÖ RaftStorageAdapter - Implements raft::Storage trait
- ‚úÖ Recovery on startup - Scans WAL segments and rebuilds index
- ‚úÖ Stored in: `data/wal/__meta/`
- ‚úÖ Survives restarts (persistent)

### 2. **Raft Networking** (v2.5.0)
- ‚úÖ GrpcTransport - Production gRPC client with connection pooling
- ‚úÖ RaftServiceImpl - gRPC server for Raft messages
- ‚úÖ Automatic peer discovery
- ‚úÖ Message routing via gRPC (not manual TCP)

### 3. **Raft Cluster Bootstrap**
- ‚úÖ RaftCluster::bootstrap() - Creates cluster with voters
- ‚úÖ Leader election loop (start_message_loop)
- ‚úÖ gRPC server startup
- ‚úÖ Multi-node peer configuration

### 4. **Metadata State Machine**
- ‚úÖ MetadataStateMachine - Stores partition assignments, ISR, leaders
- ‚úÖ Commands: CreatePartition, SetPartitionLeader, UpdateIsr
- ‚úÖ Applied from committed Raft entries
- ‚úÖ In-memory state (MetadataStateMachine)

### 5. **WAL Data Replication** (Separate from Raft)
- ‚úÖ WalReplicationManager - PostgreSQL-style streaming
- ‚úÖ Fire-and-forget from leader (never blocks produce)
- ‚úÖ Lock-free queue (crossbeam SegQueue)
- ‚úÖ TCP streaming between nodes
- ‚úÖ Heartbeat/reconnection
- ‚úÖ ACK tracking for acks=-1 (IsrAckTracker)
- ‚úÖ ISR-aware replication (only replicates to in-sync replicas)

### 6. **Integrated Kafka Server**
- ‚úÖ IntegratedKafkaServer - Wires everything together
- ‚úÖ ProduceHandler - Kafka produce with WAL persistence
- ‚úÖ FetchHandler - Kafka fetch from WAL/segments
- ‚úÖ Consumer groups - Full implementation
- ‚úÖ WAL recovery on startup - Restores high watermarks

---

## CRITICAL BUG: Cluster Crashes on Startup üö®

**Date Discovered:** 2025-11-01
**Test:** 3-node Raft cluster bootstrap
**Result:** ‚ùå **Node 2 crashed with panic after ~1 second**

### Panic Details

```
thread 'tokio-runtime-worker' panicked at raft-0.7.0/src/raw_node.rs:674:13:
not leader but has new msg after advance, raft_id: 2
```

**Location:** `raft-0.7.0/src/raw_node.rs:674` (Raft library internal validation)

### What Happened

1. Started 3 nodes with `--bootstrap` flag
2. Node 1 (id=1, Kafka:9092, Raft:9192) - Started OK
3. **Node 2 (id=2, Kafka:9093, Raft:9193) - CRASHED** after 1 second
4. Node 3 (id=3, Kafka:9094, Raft:9194) - Started OK (but couldn't form quorum)

### Root Cause

**Raft library detected protocol violation**: A non-leader node (node 2) attempted to propose messages after calling `advance()`.

This indicates a **fundamental bug in our Raft message loop** at `crates/chronik-server/src/raft_cluster.rs:450-460`.

**Likely causes:**
1. Non-leader node is calling `propose()` when it shouldn't
2. Messages are being sent to Raft before checking leadership status
3. `advance()` is being called at the wrong time in the message loop
4. Metadata commands are being proposed on non-leader nodes

### Impact

- ‚ùå **Cannot start 3-node cluster** - Crashes within 1 second
- ‚ùå **No leader election** - Cluster never reaches quorum
- ‚ùå **No metadata replication** - Can't coordinate partition assignments
- ‚ùå **No multi-node deployment** - Raft cluster mode is unusable
- ‚úÖ **Standalone mode unaffected** - Single-node works perfectly

### Reproduction

```bash
# Start 3-node cluster (crashes immediately)
./target/release/chronik-server --kafka-port 9092 --node-id 1 \
  raft-cluster --raft-addr 0.0.0.0:9192 \
  --peers "2@localhost:9193,3@localhost:9194" --bootstrap

./target/release/chronik-server --kafka-port 9093 --node-id 2 \
  raft-cluster --raft-addr 0.0.0.0:9193 \
  --peers "1@localhost:9192,3@localhost:9194" --bootstrap  # CRASHES HERE

./target/release/chronik-server --kafka-port 9094 --node-id 3 \
  raft-cluster --raft-addr 0.0.0.0:9194 \
  --peers "1@localhost:9192,2@localhost:9193" --bootstrap
```

**Test Script:** `/tmp/test_3node_cluster.sh`

### ‚úÖ FIX APPLIED (2025-11-01)

**File:** `crates/chronik-server/src/raft_cluster.rs:196-229`

**Root Cause:** The `propose()` method was calling `raft.propose()` WITHOUT checking if the node was the leader. Non-leader nodes would call propose(), violating Raft protocol.

**Fix:** Added leadership check before all Raft proposals:

```rust
pub async fn propose(&self, cmd: MetadataCommand) -> Result<()> {
    // CRITICAL FIX: Check if we're the leader BEFORE proposing
    {
        let raft = self.raft_node.read()?;
        let state = raft.raft.state;

        if state != raft::StateRole::Leader {
            return Err(anyhow::anyhow!(
                "Cannot propose: this node (id={}) is not the leader (state={:?}, leader={})",
                self.node_id, state, leader_id
            ));
        }
    }

    // Safe to propose now
    raft.propose(vec![], data)?;
    Ok(())
}
```

**Test Results:**
- ‚úÖ Nodes 2 and 3 ran for 2+ minutes without Raft panic
- ‚úÖ No more "not leader but has new msg after advance" errors
- ‚úÖ Nodes handle Kafka requests normally
- ‚ö†Ô∏è Node 1 crashed with different WAL panic (separate issue)

**Status:** **RAFT PANIC FIXED** - Cluster no longer crashes with leadership violation

### Remaining Issues

**Node 1 WAL Panic** (different issue):
```
thread 'tokio-runtime-worker' panicked at crates/chronik-wal/src/manager.rs:329:74
```

This is a separate WAL bug, not related to Raft. Investigating next.

---

## What's MISSING / CRITICAL TODOs ‚ùå

### 1. **FIX RAFT PANIC BUG** (üî¥ HIGHEST PRIORITY - BLOCKS ALL CLUSTER TESTING)

**Problem:** Non-leader nodes crash with "not leader but has new msg after advance"

**This MUST be fixed before any other Raft work!**

---

### 2. **Raft Log Persistence** (CRITICAL BLOCKER)

**Location:** `crates/chronik-server/src/raft_cluster.rs:450-460`

```rust
// Step 4: Persist entries (required before persisted_messages)
if !ready.entries().is_empty() {
    // TODO: Actually persist entries to storage
    // For now, MemStorage handles this automatically  ‚Üê THIS IS A LIE!
    tracing::debug!("Raft entries to persist: {}", ready.entries().len());
}

// Step 5: Persist hard state (required before persisted_messages)
if let Some(_hs) = ready.hs() {
    // TODO: Actually persist hard state to storage
    // For now, MemStorage handles this automatically  ‚Üê THIS IS A LIE!
    tracing::debug!("Raft hard state changed");
}
```

**Problem:** The comment says "MemStorage handles this" but we're using `RaftStorageAdapter` now, NOT MemStorage!

**Impact:**
- ‚ùå Raft entries are NOT being persisted to WAL
- ‚ùå HardState (term, voted_for, commit_index) is NOT being saved
- ‚ùå Cluster will lose state on restart
- ‚ùå May lose committed metadata commands

**Fix Required:** Call RaftWalStorage.append() and persist_hard_state() methods

---

### 2. **Raft Snapshot Persistence** (CRITICAL BLOCKER)

**Location:** `crates/chronik-server/src/raft_cluster.rs:427`

```rust
// Step 2: Apply snapshot if present
if !raft::is_empty_snap(ready.snapshot()) {
    // TODO: Persist snapshot
    tracing::debug!("Snapshot available (not persisted yet)");
}
```

**Problem:** Snapshots are never saved, so log compaction doesn't work.

**Impact:**
- ‚ùå Raft log grows unbounded
- ‚ùå No log compaction
- ‚ùå Disk fills up over time
- ‚ùå Recovery becomes slower as log grows

**Fix Required:** Implement snapshot save/load in RaftWalStorage

---

### 3. **RaftWalStorage Missing Methods**

**Location:** `crates/chronik-wal/src/raft_storage_impl.rs`

**Missing:**
- ‚ùå `append_entries()` - NOT called from message loop
- ‚ùå `persist_hard_state()` - Doesn't exist
- ‚ùå `apply_snapshot()` - Doesn't exist
- ‚ùå `create_snapshot()` - Doesn't exist

**Current State:** RaftWalStorage has the infrastructure but the message loop doesn't call it!

---

### 4. **No Integration Testing**

**Status:** ZERO integration tests for Raft cluster

**Missing Tests:**
- ‚ùå 3-node cluster bootstrap
- ‚ùå Leader election
- ‚ùå Metadata replication (create topic on leader, verify on follower)
- ‚ùå Crash recovery (kill node, restart, verify metadata restored)
- ‚ùå Network partition handling
- ‚ùå Follower lag detection

---

### 5. **Cluster Config Not Wired Up**

**Location:** `crates/chronik-server/src/raft_cluster.rs:611`

```rust
cluster_config: None,  // TODO(Phase 3): Wire RaftCluster to cluster_config
```

**Problem:** IntegratedKafkaServer doesn't know about RaftCluster

**Impact:**
- ‚ùå Can't query partition leaders from Raft metadata
- ‚ùå Can't update ISR via Raft
- ‚ùå ProduceHandler doesn't know which node is leader
- ‚ùå No partition assignment coordination

---

### 6. **No Partition Assignment on Topic Creation**

**Problem:** When a topic is created, who assigns partitions to nodes?

**Current State:** Topics are created locally, no Raft coordination

**Missing:**
- ‚ùå CreateTopic ‚Üí Raft proposal
- ‚ùå Partition assignment algorithm (which nodes get which partitions?)
- ‚ùå Leader election per partition
- ‚ùå Metadata propagation to all nodes

---

### 7. **No ISR Updates via Raft**

**Problem:** ISR (In-Sync Replicas) is tracked locally, not coordinated via Raft

**Impact:**
- ‚ùå Different nodes have different ISR state
- ‚ùå No consensus on which replicas are in-sync
- ‚ùå Can't safely elect new partition leaders

**Fix Required:** Send ISR updates as Raft proposals (UpdateIsr command)

---

### 8. **No Leader Forwarding**

**Problem:** What happens when a client sends produce to a follower?

**Current State:** Likely fails or writes locally (wrong!)

**Required:**
- ‚ùå Detect if this node is partition leader
- ‚ùå If not, forward request to leader node
- ‚ùå Return error to client with leader info

---

## Production Readiness Checklist

### Tier 0: üî¥ CRITICAL CRASH BUG (Fix FIRST before anything else)

- [x] **FIX RAFT PANIC: "not leader but has new msg after advance"** ‚úÖ **FIXED 2025-11-01**
  - Status: ‚úÖ **FIXED** - Nodes 2 and 3 run without Raft panic
  - Impact: Was crashing in 1 second, now runs indefinitely
  - Fix: Added leadership check in `propose()` method
  - File: `crates/chronik-server/src/raft_cluster.rs:196-229`
  - Remaining: Node 1 has different WAL panic (separate issue)

### Tier 1: CRITICAL BLOCKERS (Must fix before ANY testing)

- [x] **Fix Raft entry persistence** ‚úÖ **FIXED 2025-11-01**
  - Was: Spawned in background, never awaited
  - Now: AWAITs `storage.append_entries()` before `advance()`
  - File: `crates/chronik-server/src/raft_cluster.rs:475-491`

- [x] **Fix HardState persistence** ‚úÖ **FIXED 2025-11-01**
  - Was: Spawned in background, never awaited
  - Now: AWAITs `storage.persist_hard_state()` before `advance()`
  - File: `crates/chronik-server/src/raft_cluster.rs:493-507`

- [x] **Fix WAL recovery panic** ‚úÖ **FIXED 2025-11-01**
  - Was: Called `.unwrap()` on truncated file reads, caused crashes
  - Now: Gracefully handles `UnexpectedEof`, skips rest of file
  - File: `crates/chronik-wal/src/manager.rs:319-388`

- [ ] **Implement snapshot save/load** - Prevent unbounded log growth
- [ ] **Wire RaftCluster to IntegratedServer** - Connect cluster_config

### Tier 2: CORE FUNCTIONALITY (Must fix for multi-node cluster)

- [ ] **Partition assignment on topic creation** - Raft-coordinated CreateTopic
- [ ] **ISR updates via Raft** - UpdateIsr command with Raft consensus
- [ ] **Partition leader election** - Automatic failover when leader dies
- [ ] **Leader forwarding** - Route produce requests to partition leader

### Tier 3: TESTING (Must pass before production)

- [ ] **3-node cluster bootstrap test** - Verify all nodes start, elect leader
- [ ] **Metadata replication test** - Create topic on leader, verify on followers
- [ ] **Crash recovery test** - Kill node, restart, verify metadata/data intact
- [ ] **Leader election test** - Kill leader, verify follower promoted
- [ ] **WAL replication test** - Produce to leader, consume from follower
- [ ] **Network partition test** - Simulate split-brain, verify recovery

### Tier 4: PRODUCTION HARDENING (Before real users)

- [ ] **Monitoring** - Expose Raft metrics (leader ID, log size, commit index)
- [ ] **Observability** - Trace Raft proposals through consensus
- [ ] **Backpressure** - Limit Raft proposal queue size
- [ ] **Graceful shutdown** - Persist state before exit
- [ ] **Membership changes** - Add/remove nodes from cluster
- [ ] **Log compaction** - Automatic snapshot creation after N entries
- [ ] **Follower read consistency** - Read-your-writes guarantees

---

## Estimated Work Remaining

### Tier 0 (Critical Crash): **1-2 days** üî¥
- Debug Raft panic bug: 0.5-1 day
- Fix message loop logic: 0.5-1 day

### Tier 1 (Blockers): **2-3 days**
- Fix entry/hardstate persistence: 1 day
- Implement snapshot save/load: 1 day
- Wire RaftCluster to server: 0.5 days

### Tier 2 (Core): **3-5 days**
- Partition assignment: 1 day
- ISR coordination: 1 day
- Leader election: 1 day
- Leader forwarding: 1 day

### Tier 3 (Testing): **3-5 days**
- Write tests: 2 days
- Debug failures: 2-3 days

### Tier 4 (Hardening): **5-7 days**
- Monitoring: 1 day
- Membership changes: 2 days
- Log compaction: 1 day
- Production testing: 2-3 days

**Total:** 15-22 days of focused work (including crash fix)

---

## Does It Even Work Right Now?

**Short answer:** üî¥ **NO - CRASHES IMMEDIATELY**

**Test Results (2025-11-01):**

‚úÖ **Standalone Mode:**
- ‚úÖ Server starts without crashing
- ‚úÖ Kafka produce/fetch works perfectly
- ‚úÖ rdkafka consumers work (tested with chronik-bench)
- ‚úÖ OffsetCommit v8 works correctly
- ‚úÖ Consumer groups fully functional
- ‚úÖ WAL persistence and recovery work
- **VERDICT:** ‚úÖ **Production-ready for single-node deployments**

‚ùå **3-Node Raft Cluster:**
- ‚ùå **Crashes in 1 second** with Raft panic
- ‚ùå Node 2 dies: "not leader but has new msg after advance"
- ‚ùå Cannot form quorum (2 nodes alive, 1 crashed)
- ‚ùå No leader election completes
- ‚ùå Cluster is completely unusable
- **VERDICT:** üî¥ **BROKEN - Do not use**

**What works:**
- ‚úÖ Single-node standalone mode (fully tested)
- ‚úÖ Kafka protocol compatibility
- ‚úÖ WAL-based persistence
- ‚úÖ Consumer groups and offset commits

**What crashes:**
- ‚ùå 3-node cluster bootstrap
- ‚ùå Raft leader election
- ‚ùå Multi-node metadata coordination
- ‚ùå Cluster mode is completely broken

---

## Recommended Next Steps

### IMMEDIATE (Next 2 days)

1. üî¥ **FIX THE CRASH BUG FIRST** - Debug Raft panic in message loop
   - Enable `RUST_BACKTRACE=1` and `RUST_LOG=trace`
   - Review `start_message_loop()` and `process_ready()` ordering
   - Add leadership checks before all `propose()` calls
   - Study raft-rs examples for correct usage

2. **Run the existing test again** - Verify crash is fixed
   - Use `/tmp/test_3node_cluster.sh`
   - Should complete without panic

### AFTER CRASH FIX

3. **Fix Tier 1 blockers** (entry/hardstate persistence)
4. **Add more integration tests** - Metadata replication, crash recovery
5. **Fix failures iteratively** - Test, fix, test, fix
6. **Production hardening** - Monitoring, graceful shutdown, etc.

**DO NOT:**
- ‚ùå **Work on ANY other features** until crash is fixed
- ‚ùå Add more Raft code before fixing the panic
- ‚ùå Assume it works without running `/tmp/test_3node_cluster.sh`
- ‚ùå Deploy cluster mode to production (guaranteed crash)

---

## Conclusion

**Standalone Mode:** ‚úÖ **PRODUCTION-READY** - Fully tested, works perfectly.

**Cluster Mode:** üî¥ **BROKEN** - Crashes on startup, unusable.

**Can we ship standalone?** ‚úÖ **YES** - Single-node deployments are solid.

**Can we ship cluster?** ‚ùå **ABSOLUTELY NOT** - Guaranteed crash within 1 second.

**How much work to fix cluster?** üìÖ **3-4 weeks** (crash fix + persistence + testing)

**Priority:** üî¥ **FIX RAFT PANIC FIRST** - Nothing else matters until cluster starts.

**Should we continue?** ‚úÖ **Yes** - Architecture is sound, but crash bug MUST be fixed before any other work.
