# v2.2-v2.3 Performance Regression Analysis & Reset

**Date**: 2025-10-31
**Action**: Reset main branch to v2.1.0 clean state
**Reason**: WAL-Batch-to-Raft implementation broke performance (78K → 1.2K msg/s = 98.5% regression)

---

## Executive Summary

Between v2.1.0 (October 29) and v2.3.0 (October 30), we implemented WAL-Batch-to-Raft for multi-node replication. While architecturally sound, the implementation introduced **catastrophic performance regression** in standalone mode:

- **v2.1.0 baseline**: 78,544 msg/s @ 128 concurrency (verified 2025-10-31)
- **v2.3.0 actual**: 1,174 msg/s @ 128 concurrency (98.5% regression!)
- **Target**: Restore 78K msg/s baseline AND add WAL replication cleanly

---

## What Went Wrong

### Root Causes Identified

1. **RwLock in Hot Path** (`produce_handler.rs:333`)
   - Wrapped `raft_manager` in `Arc<RwLock<Option<Arc<>>>>` for "interior mutability"
   - Every produce request acquired `.read().await` lock
   - 128 concurrent producers = serialization bottleneck
   - Fix attempt improved to 18.5K msg/s but still 76% slower than baseline

2. **Missing pending_batches Population** (`produce_handler.rs:1223-1330`)
   - New WAL replication path writes to WAL but returns early
   - Skips `pending_batches` buffering that v2.1.0 had
   - ProduceFlushProfile logic bypassed entirely
   - Consumers can't read from memory cache, must hit WAL

3. **Flush Logic Not Called**
   - v2.1.0: Every produce calls `flush_partition_if_needed()` at end
   - v2.3.0: New path returns early, never flushes
   - HighThroughput profile (100 batches) not utilized
   - Breaks the two-layer batching optimization

### Performance Comparison

| Metric | v2.1.0 (Baseline) | v2.3.0 (Broken) | v2.4.1 (Attempted Fix) |
|--------|-------------------|-----------------|------------------------|
| Throughput | 78,544 msg/s | 1,174 msg/s (-98.5%) | 3,885 msg/s (-95.1%) |
| p99 Latency | 4.89ms | ~2,824ms (+57,600%) | 192ms (+3,830%) |
| Hot path locks | 0 | 128/batch | 0 (fixed) |
| pending_batches | ✅ Used | ❌ Skipped | ✅ Added back |
| Flush profile | ✅ Utilized | ❌ Bypassed | ✅ Called |

**Conclusion**: Even with all fixes, still 95% slower than baseline. The architecture needs complete redesign.

---

## Commits Archived

Created archive branch: `archive/failed-raft-data-replication-v2.2-v2.3`

Archived commits (newest to oldest):
```
63aabb0 bug(raft): Critical - WalManager missing batch notifier in Raft mode
4369a90 test(bench): Add 128 concurrency stress test results
92ef040 test(bench): Add chronik-bench results for v2.3.0 standalone mode
4a5571c docs(raft): Add implementation completion summary for v2.3.0
c1878b8 feat(raft): Phase 5-8: Complete WAL-Batch-to-Raft implementation
69c60ed docs: Add comprehensive session summary for WAL-Batch-to-Raft
f43f0ea feat(raft): Phase 4: Refactor produce handler for batched Raft mode
e1de672 feat(raft): Phase 3: Implement RaftBatchProposer worker
cc2459a feat(wal): Phase 1-2: Add batch notification infrastructure
98031a5 docs(raft): Add implementation plan and checklist
b5173f7 feat(raft): Add comprehensive latency instrumentation
2723d5c perf(raft): Optimize gRPC startup, WAL batching, and peer messaging
```

---

## Lessons Learned

### What NOT to Do

1. ❌ **Never wrap hot path fields in RwLock**
   - Use `Option<Arc<>>` and set once during initialization
   - Or pass dependencies during construction
   - Interior mutability via RwLock = guaranteed bottleneck

2. ❌ **Never add early returns that skip critical logic**
   - v2.3.0 added early return that bypassed `pending_batches` buffering
   - Broke the two-layer batching optimization (ProduceFlushProfile + GroupCommitWal)
   - Always trace full execution path when refactoring

3. ❌ **Never assume performance without testing**
   - v2.3.0 claimed "270x improvement" without actual benchmarks
   - Reality: 98.5% regression!
   - **ALWAYS test before releasing performance claims**

### What TO Do

1. ✅ **Verify baseline FIRST**
   - Confirmed v2.1.0: 78.5K msg/s @ 128 concurrency (2025-10-31)
   - Gives us clear regression detection

2. ✅ **Archive failed attempts, don't delete**
   - Mistakes contain valuable lessons
   - Code might have useful patterns even if architecture failed

3. ✅ **Fix forward from clean state**
   - Reset main to known-good v2.1.0
   - Reimplement WAL replication cleanly
   - Test at each step to catch regressions early

---

## Path Forward: Clean WAL Replication Implementation

### Design Principles (Non-Negotiable)

1. **Zero locks in produce hot path**
   - All dependencies set during initialization
   - Use `Option<Arc<>>`, never `Arc<RwLock<>>`

2. **Preserve v2.1.0 produce flow**
   - Keep `pending_batches` buffering
   - Keep `flush_partition_if_needed()` call
   - Keep ProduceFlushProfile optimization

3. **Add WAL replication as OPTIONAL side effect**
   - Write to local WAL first (existing path)
   - Buffer to `pending_batches` (existing path)
   - Flush based on profile (existing path)
   - **Then** optionally replicate if replication manager exists
   - Replication happens in background, NOT blocking produce

### Implementation Plan

**Phase 1: Minimal WAL Replication (Target: Maintain 78K msg/s)**

1. Add `WalReplicationManager` field to `ProduceHandler`
   - Type: `Option<Arc<WalReplicationManager>>` (NO RwLock!)
   - Set during `ProduceHandler::new()` construction
   - Never modified after initialization

2. Add replication hook AFTER flush
   ```rust
   // After flush_partition_if_needed()
   if let Some(ref repl_mgr) = self.replication_manager {
       // Fire-and-forget replication (tokio::spawn)
       // Don't block produce path!
       repl_mgr.replicate_async(topic, partition, wal_record).await;
   }
   ```

3. Test at each step
   - After Step 1: Verify 78K msg/s maintained
   - After Step 2: Verify replication works AND 78K maintained

**Phase 2: PostgreSQL-Style Streaming (If Phase 1 succeeds)**

Only proceed if Phase 1 maintains baseline performance.

### Success Criteria

- ✅ Standalone mode: >= 75K msg/s @ 128 concurrency (within 5% of v2.1.0)
- ✅ With replication: >= 70K msg/s @ 128 concurrency (within 10% acceptable)
- ✅ Clean code: No RwLocks in hot paths
- ✅ Tested: Benchmarks run and documented

### Failure Criteria (Stop and Reconsider)

- ❌ Any throughput < 50K msg/s
- ❌ Any p99 latency > 20ms
- ❌ Any degradation over time (throttling pattern)

---

## Benchmark Results

### v2.1.0 Baseline (Verified 2025-10-31)

```
╔══════════════════════════════════════════════════════════════╗
║            Chronik v2.1.0 Baseline Results                  ║
╠══════════════════════════════════════════════════════════════╣
║ Messages:            2,356,418 total
║ Failed:                      0 (0.00%)
║ Message rate:           78,544 msg/s ✅
║ Bandwidth:               19.18 MB/s
╠══════════════════════════════════════════════════════════════╣
║ p50:                     1.48 ms
║ p90:                     1.94 ms
║ p95:                     2.21 ms
║ p99:                     4.89 ms ✅
║ p99.9:                  10.94 ms
║ max:                    25.73 ms
╚══════════════════════════════════════════════════════════════╝
```

**Test Command**:
```bash
./target/release/chronik-bench \
  --bootstrap-servers localhost:9092 \
  --topic chronik-v2.1.0-baseline \
  --mode produce \
  --concurrency 128 \
  --duration 30s \
  --message-size 256 \
  --create-topic
```

---

## References

- Baseline performance: `archive/CHRONIK_PERFORMANCE_FINAL_2025-10-29.md`
- Failed implementation: `archive/failed-raft-data-replication-v2.2-v2.3` branch
- Regression analysis: `docs/V2.4.1_RWLOCK_PERFORMANCE_FIX.md`
- Clean slate: main branch @ e223eff (v2.1.0)
