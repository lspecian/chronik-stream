# Chronik Stream

[![Build Status](https://github.com/lspecian/chronik-stream/workflows/CI/badge.svg)](https://github.com/lspecian/chronik-stream/actions)
[![Release](https://img.shields.io/github/v/release/lspecian/chronik-stream)](https://github.com/lspecian/chronik-stream/releases)
[![Docker Image](https://img.shields.io/badge/docker-ghcr.io-blue)](https://github.com/lspecian/chronik-stream/pkgs/container/chronik-stream)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/rust-1.75%2B-orange.svg)](https://www.rust-lang.org)

A high-performance streaming platform built in Rust that implements core Kafka wire protocol functionality with comprehensive Write-Ahead Log (WAL) durability and automatic recovery.

**Latest Release: v2.2.17** - Major codebase cleanup. See [CHANGELOG.md](CHANGELOG.md) for full release history.

## âœ¨ What's New in v2.2.17

ğŸ§¹ **Major Cleanup**: Removed 547 obsolete files (~170K lines) for a leaner codebase
ğŸ“š **Updated Docs**: Reduced docs from 246 to 19 essential files, updated for current CLI
ğŸ”§ **Simplified Scripts**: Reduced from 26 to 5 essential build/health scripts
ğŸ“¦ **Removed Obsolete Crates**: chronik-admin, chronik-benchmarks, chronik-cli (functionality integrated)

**Upgrade Recommendation**: All users should upgrade to v2.2.17 for the cleanest codebase.

## ğŸš€ Features

- **Kafka Wire Protocol**: Full Kafka wire protocol with consumer group and transactional support
- **Searchable Topics**: Opt-in real-time full-text search with Tantivy (3% overhead) - see [docs/SEARCHABLE_TOPICS.md](docs/SEARCHABLE_TOPICS.md)
- **Full Compression Support**: All Kafka compression codecs (Gzip, Snappy, LZ4, Zstd) - see [COMPRESSION_SUPPORT.md](COMPRESSION_SUPPORT.md)
- **WAL-based Metadata**: ChronikMetaLog provides event-sourced metadata persistence
- **GroupCommitWal**: PostgreSQL-style group commit with per-partition background workers and batched fsync
- **Zero Message Loss**: WAL ensures durability for all acks modes (0, 1, -1) even during unexpected shutdowns
- **Automatic Recovery**: WAL records are automatically replayed on startup to restore state with 100% accuracy
- **Real Client Testing**: Tested with kafka-python, confluent-kafka, KSQL, and Apache Flink
- **Stress Tested**: Verified at scale with millions of messages, zero duplicates, 300K+ msgs/sec throughput
- **Transactional APIs**: Full support for Kafka transactions (InitProducerId, AddPartitionsToTxn, EndTxn)
- **High Performance**: Async architecture with zero-copy networking optimizations
- **Multi-Architecture**: Native support for x86_64 and ARM64 (Apple Silicon, AWS Graviton)
- **Container Ready**: Docker deployment with proper network configuration
- **Simplified Operations**: Single-process architecture reduces operational complexity

## ğŸ—ï¸ Architecture - 3-Tier Seamless Storage

Chronik implements a unique 3-tier storage system with automatic failover that provides **infinite retention** without requiring infinite local disk:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Chronik 3-Tier Seamless Storage                     â”‚
â”‚                   (Infinite Retention Design)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tier 1: WAL (Hot - Local Disk)                                 â”‚
â”‚  â”œâ”€ Location: ./data/wal/{topic}/{partition}/                   â”‚
â”‚  â”œâ”€ Latency: <1ms (in-memory buffer)                            â”‚
â”‚  â””â”€ Retention: Until sealed (250MB or 30min by default)         â”‚
â”‚        â†“ Background WalIndexer (every 30s)                       â”‚
â”‚                                                                   â”‚
â”‚  Tier 2: Raw Segments in S3 (Warm - Object Storage)             â”‚
â”‚  â”œâ”€ Location: s3://bucket/segments/{topic}/{partition}/{range}  â”‚
â”‚  â”œâ”€ Latency: 50-200ms (download + deserialize)                  â”‚
â”‚  â”œâ”€ Retention: Unlimited (cheap object storage)                 â”‚
â”‚  â””â”€ Purpose: Message consumption after local WAL deletion        â”‚
â”‚        â†“ PLUS â†“                                                  â”‚
â”‚                                                                   â”‚
â”‚  Tier 3: Tantivy Indexes in S3 (Cold - Searchable)              â”‚
â”‚  â”œâ”€ Location: s3://bucket/indexes/{topic}/partition-{p}/...     â”‚
â”‚  â”œâ”€ Latency: 100-500ms (download + decompress + search)         â”‚
â”‚  â”œâ”€ Retention: Unlimited                                         â”‚
â”‚  â””â”€ Purpose: Full-text search WITHOUT downloading raw data       â”‚
â”‚                                                                   â”‚
â”‚  Consumer Fetch Flow (Automatic Fallback):                      â”‚
â”‚    Phase 1: Try WAL buffer (hot, in-memory) â†’ Î¼s latency        â”‚
â”‚    Phase 2: Try local WAL (warm, local disk) â†’ ms latency       â”‚
â”‚    Phase 3: Download raw segment from S3 â†’ 50-200ms latency     â”‚
â”‚    Phase 4: Search Tantivy index â†’ 100-500ms latency            â”‚
â”‚                                                                   â”‚
â”‚  Local Disk Cleanup:                                             â”‚
â”‚    - WAL files DELETED after successful upload to S3             â”‚
â”‚    - Old messages still accessible from S3 indefinitely          â”‚
â”‚    - No infinite local disk space required!                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Kafka Client  â”‚  (kafka-python, Java clients, KSQL, etc.)
    â”‚  (Any Language) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Chronik Server                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ Kafka Proto  â”‚  â”‚ ChronikMetaLog  â”‚ â”‚
    â”‚  â”‚ Handler      â”‚  â”‚ (WAL Metadata)  â”‚ â”‚
    â”‚  â”‚ (Port 9092)  â”‚  â”‚                 â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚   Search     â”‚  â”‚  Storage Mgr    â”‚ â”‚
    â”‚  â”‚  (Tantivy)   â”‚  â”‚  (3-Tier)       â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Object Storage         â”‚
    â”‚  (S3/GCS/Azure/Local)     â”‚
    â”‚  â€¢ Raw segments (Tier 2)  â”‚
    â”‚  â€¢ Tantivy indexes (Tier 3)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Differentiators vs Kafka Tiered Storage

| Feature | Kafka Tiered Storage | Chronik Layered Storage |
|---------|---------------------|-------------------------|
| **Hot Storage** | Local disk | WAL + Segments (local) |
| **Cold Storage** | S3 (raw data) | S3 raw segments + Tantivy indexes |
| **Auto-archival** | âœ… Yes | âœ… Yes (WalIndexer background task) |
| **Query by Offset** | âœ… Yes | âœ… Yes (download from S3 as needed) |
| **Full-text Search** | âŒ NO | âœ… **YES** (Tantivy indexes, no download!) |
| **Local Disk** | Grows forever | Bounded (old WAL deleted after S3 upload) |

**Unique Advantage**: Chronik's Tier 3 isn't just "cold storage" - it's a **searchable indexed archive**. You can query old data by content or timestamp range without downloading or scanning raw data!

## âš¡ Quick Start

### Using Docker (Recommended)

```bash
# Quick start - single node
docker run -d -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  ghcr.io/lspecian/chronik-stream:latest start

# With persistent storage
docker run -d --name chronik \
  -p 9092:9092 \
  -v chronik-data:/data \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e RUST_LOG=info \
  ghcr.io/lspecian/chronik-stream:latest start

# Using docker-compose
curl -O https://raw.githubusercontent.com/lspecian/chronik-stream/main/docker-compose.yml
docker-compose up -d
```

### With S3/MinIO Object Storage

```bash
# MinIO for development
docker run -d --name chronik \
  -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e OBJECT_STORE_BACKEND=s3 \
  -e S3_ENDPOINT=http://minio:9000 \
  -e S3_BUCKET=chronik-storage \
  -e S3_ACCESS_KEY=minioadmin \
  -e S3_SECRET_KEY=minioadmin \
  -e S3_PATH_STYLE=true \
  ghcr.io/lspecian/chronik-stream:latest start

# AWS S3 for production (uses IAM role)
docker run -d --name chronik \
  -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e OBJECT_STORE_BACKEND=s3 \
  -e S3_REGION=us-west-2 \
  -e S3_BUCKET=chronik-prod-archives \
  ghcr.io/lspecian/chronik-stream:latest start
```

### âš ï¸ Critical Docker Configuration

**IMPORTANT**: When running Chronik Stream in Docker or binding to `0.0.0.0`, you **MUST** set `CHRONIK_ADVERTISED_ADDR`:

```yaml
# docker-compose.yml example
services:
  chronik-stream:
    image: ghcr.io/lspecian/chronik-stream:latest
    ports:
      - "9092:9092"
    environment:
      CHRONIK_BIND_ADDR: "0.0.0.0"  # Just host, no port
      CHRONIK_ADVERTISED_ADDR: "chronik-stream"  # REQUIRED - use container name for Docker networks
      # or "localhost" for host access, or your public hostname/IP for remote access
```

Without `CHRONIK_ADVERTISED_ADDR`, clients will receive `0.0.0.0:9092` in metadata responses and fail to connect.

### Test with Kafka Client

```python
# Python example
from kafka import KafkaProducer, KafkaConsumer

# Single-node setup
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    api_version=(0, 10, 0)  # Important: specify version
)
producer.send('test-topic', b'Hello Chronik!')
producer.flush()

# Consumer
consumer = KafkaConsumer(
    'test-topic',
    bootstrap_servers='localhost:9092',
    api_version=(0, 10, 0),
    auto_offset_reset='earliest'
)
for message in consumer:
    print(f"Received: {message.value}")
```

**âš ï¸ CRITICAL for Cluster Deployments**: When using a multi-node cluster, **ALWAYS configure clients with ALL cluster brokers** for 100% message consumption success:

```python
# âœ… CORRECT - Cluster configuration (ALL brokers)
producer = KafkaProducer(
    bootstrap_servers='localhost:9092,localhost:9093,localhost:9094',  # All 3 brokers!
    api_version=(0, 10, 0)
)

# âŒ WRONG - Single broker causes leadership rejections and message loss
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',  # Only one broker - NOT RECOMMENDED for clusters!
    api_version=(0, 10, 0)
)
```

See [docs/100_PERCENT_CONSUMPTION_INVESTIGATION.md](docs/100_PERCENT_CONSUMPTION_INVESTIGATION.md) for detailed analysis.

### Using Binary

```bash
# Download latest release (Linux x86_64)
curl -L https://github.com/lspecian/chronik-stream/releases/latest/download/chronik-server-linux-amd64.tar.gz -o chronik-server.tar.gz
tar xzf chronik-server.tar.gz
./chronik-server start

# macOS (Apple Silicon)
curl -L https://github.com/lspecian/chronik-stream/releases/latest/download/chronik-server-darwin-arm64.tar.gz -o chronik-server.tar.gz
tar xzf chronik-server.tar.gz
./chronik-server start
```

### Building from Source

```bash
# Clone repository
git clone https://github.com/lspecian/chronik-stream.git
cd chronik-stream

# Build release binary
cargo build --release --bin chronik-server

# Run single-node
./target/release/chronik-server start

# Or run 3-node cluster locally
./target/release/chronik-server start --config config/examples/cluster/chronik-cluster-node1.toml
./target/release/chronik-server start --config config/examples/cluster/chronik-cluster-node2.toml
./target/release/chronik-server start --config config/examples/cluster/chronik-cluster-node3.toml
```

## ğŸŒŸ KSQL Integration

Chronik Stream provides **full compatibility** with KSQLDB (Confluent's SQL engine for Kafka) including transactional support. Simply point KSQLDB at Chronik's Kafka endpoint:

```properties
# ksql-server.properties
bootstrap.servers=localhost:9092
ksql.service.id=ksql_service_1
```

For detailed KSQL setup and usage examples, see [docs/KSQL_INTEGRATION_GUIDE.md](docs/KSQL_INTEGRATION_GUIDE.md).

## ğŸ¯ Operational Modes

The unified `chronik-server` binary supports two deployment modes via the `start` command:

### Single-Node Mode (Default)
Perfect for development, testing, and single-node production deployments:

```bash
# Simplest - just start
./chronik-server start

# With custom data directory
./chronik-server start --data-dir /var/lib/chronik

# With advertised address (required for Docker/remote clients)
./chronik-server start --advertise my-hostname.com:9092
```

**Features:**
- âœ… Full Kafka protocol compatibility
- âœ… WAL-based durability (zero message loss)
- âœ… Automatic crash recovery
- âœ… 3-tier storage (local + S3/GCS/Azure)
- âœ… Full-text search with Tantivy

### Cluster Mode (Multi-Node Replication)
**Available in v2.2.0+**: Production-ready multi-node cluster with Raft consensus, automatic replication, and zero-downtime operations.

**Minimum 3 nodes required** for quorum-based replication.

**Quick Start (Local Testing):**
```bash
# Terminal 1 - Node 1
./chronik-server start --config config/examples/cluster/chronik-cluster-node1.toml

# Terminal 2 - Node 2
./chronik-server start --config config/examples/cluster/chronik-cluster-node2.toml

# Terminal 3 - Node 3
./chronik-server start --config config/examples/cluster/chronik-cluster-node3.toml
```

**Production Setup (3 Machines):**
```bash
# On each node, create config file with unique node_id
# Example node1.toml:
enabled = true
node_id = 1
replication_factor = 3
min_insync_replicas = 2

[[peers]]
id = 1
kafka = "node1.example.com:9092"
wal = "node1.example.com:9291"
raft = "node1.example.com:5001"

[[peers]]
id = 2
kafka = "node2.example.com:9092"
wal = "node2.example.com:9291"
raft = "node2.example.com:5001"

[[peers]]
id = 3
kafka = "node3.example.com:9092"
wal = "node3.example.com:9291"
raft = "node3.example.com:5001"

# Start each node
./chronik-server start --config /etc/chronik/node1.toml
```

**Key Features:**
- âœ… Quorum-based replication (survives minority node failures)
- âœ… Automatic leader election via Raft consensus
- âœ… Strong consistency (linearizable reads/writes)
- âœ… Zero-downtime node addition and removal (v2.2.0+)
- âœ… Automatic partition rebalancing
- âœ… Comprehensive monitoring via Prometheus metrics

**Cluster Management:**
```bash
# Add node to running cluster
export CHRONIK_ADMIN_API_KEY=<key>
./chronik-server cluster add-node 4 \
  --kafka node4:9092 \
  --wal node4:9291 \
  --raft node4:5001 \
  --config cluster.toml

# Query cluster status
./chronik-server cluster status --config cluster.toml

# Remove node gracefully
./chronik-server cluster remove-node 4 --config cluster.toml
```

**Complete Guide:** See [docs/RUNNING_A_CLUSTER.md](docs/RUNNING_A_CLUSTER.md) for step-by-step instructions.

### Configuration Options

**Commands:**
```bash
chronik-server start [OPTIONS]          # Start server (auto-detects single-node or cluster)
chronik-server cluster <SUBCOMMAND>     # Manage cluster (add-node, remove-node, status)
chronik-server version                  # Display version info
chronik-server compact <SUBCOMMAND>     # Manage WAL compaction
```

**Start Command Options:**
```bash
chronik-server start [OPTIONS]

Options:
  -d, --data-dir <DIR>         Data directory (default: ./data)
  --config <FILE>              Cluster config file (enables cluster mode)
  --node-id <ID>               Override node ID from config
  --advertise <ADDR:PORT>      Advertised Kafka address (for remote clients)
  -l, --log-level <LEVEL>      Log level (error/warn/info/debug/trace)
```

**Key Environment Variables:**
```bash
# Server Configuration
CHRONIK_DATA_DIR             Data directory path (default: ./data)
CHRONIK_ADVERTISED_ADDR      Address advertised to clients (CRITICAL for Docker)
RUST_LOG                     Log level (error, warn, info, debug, trace)

# Performance Tuning
CHRONIK_WAL_PROFILE          WAL performance: low/medium/high/ultra (auto-detects)
CHRONIK_PRODUCE_PROFILE      Producer flush: low-latency/balanced/high-throughput
CHRONIK_WAL_ROTATION_SIZE    WAL segment size: 100KB/250MB (default)/1GB

# Cluster Management (v2.2.0+)
CHRONIK_ADMIN_API_KEY        Admin API authentication key (REQUIRED for production clusters)

# Object Store (3-Tier Storage)
OBJECT_STORE_BACKEND         Backend: s3/gcs/azure/local (default: local)

# S3/MinIO Configuration
S3_ENDPOINT                  S3 endpoint (e.g., http://minio:9000)
S3_REGION                    AWS region (default: us-east-1)
S3_BUCKET                    Bucket name (default: chronik-storage)
S3_ACCESS_KEY                Access key ID
S3_SECRET_KEY                Secret access key
S3_PATH_STYLE                Path-style URLs (default: true, required for MinIO)
S3_DISABLE_SSL               Disable SSL (default: false)

# GCS Configuration
GCS_BUCKET                   GCS bucket name
GCS_PROJECT_ID               GCP project ID

# Azure Configuration
AZURE_ACCOUNT_NAME           Storage account name
AZURE_CONTAINER              Container name
```

## âš¡ Performance Tuning

Chronik Stream provides two layers of performance tuning for different workloads:

### WAL Performance Profiles

The Write-Ahead Log is the primary performance lever. It automatically detects system resources (CPU, memory, Docker/K8s limits) and selects an appropriate profile. Override with:

```bash
CHRONIK_WAL_PROFILE=low        # Containers, small VMs (â‰¤1 CPU, <512MB) - 2ms batch
CHRONIK_WAL_PROFILE=medium     # Typical servers (2-4 CPUs, 512MB-4GB) - 10ms batch
CHRONIK_WAL_PROFILE=high       # Dedicated servers (4-16 CPUs, 4-16GB) - 50ms batch
CHRONIK_WAL_PROFILE=ultra      # Maximum throughput (16+ CPUs, 16GB+) - 100ms batch
```

**Benchmark results use `high` profile** - see [BASELINE_PERFORMANCE.md](BASELINE_PERFORMANCE.md) for detailed methodology.

### Producer Flush Profiles

Control when buffered messages become visible to consumers:

| Profile | Batches | Flush Interval | Buffer | Use Case |
|---------|---------|----------------|--------|----------|
| `low-latency` (default) | 1 | 10ms | 16MB | Real-time analytics, instant messaging |
| `balanced` | 10 | 100ms | 32MB | General-purpose workloads |
| `high-throughput` | 100 | 500ms | 128MB | Data pipelines, ETL, batch processing |
| `extreme` | 500 | 2000ms | 512MB | Bulk ingestion, data migrations |

```bash
# Set producer profile (low-latency is default, use high-throughput for batch workloads)
CHRONIK_PRODUCE_PROFILE=high-throughput ./chronik-server start
```

### Benchmarking

Run the built-in benchmark tool:
```bash
cargo build --release
./target/release/chronik-bench -c 128 -s 256 -d 30s -m produce
```

## ğŸ“¦ Docker Images

All images support both **linux/amd64** and **linux/arm64** architectures:

| Image | Tags | Description |
|-------|------|-------------|
| `ghcr.io/lspecian/chronik-stream` | `latest`, `v2.2.17`, `2.2` | Chronik server with full KSQL support |

### Supported Platforms

- âœ… **Linux x86_64** (amd64)
- âœ… **Linux ARM64** (aarch64) - AWS Graviton, Raspberry Pi 4+
- âœ… **macOS x86_64** (Intel)
- âœ… **macOS ARM64** (Apple Silicon M1/M2/M3)

## âœ… Kafka Compatibility

### Supported Kafka APIs (24 total)

| API | Version | Status | Description |
|-----|---------|--------|-------------|
| Produce | v0-v9 | âœ… Full | Send messages to topics |
| Fetch | v0-v13 | âœ… Full | Retrieve messages from topics |
| ListOffsets | v0-v7 | âœ… Full | Query partition offsets |
| Metadata | v0-v12 | âœ… Full | Get cluster metadata |
| OffsetCommit | v0-v8 | âœ… Full | Commit consumer offsets |
| OffsetFetch | v0-v8 | âœ… Full | Retrieve consumer offsets |
| FindCoordinator | v0-v4 | âœ… Full | Find group coordinator |
| JoinGroup | v0-v9 | âœ… Full | Join consumer group |
| Heartbeat | v0-v4 | âœ… Full | Consumer heartbeat |
| LeaveGroup | v0-v5 | âœ… Full | Leave consumer group |
| SyncGroup | v0-v5 | âœ… Full | Sync group assignments |
| ApiVersions | v0-v3 | âœ… Full | Negotiate API versions |
| CreateTopics | v0-v7 | âœ… Full | Create new topics |
| DeleteTopics | v0-v6 | âœ… Full | Delete topics |
| DescribeGroups | v0-v5 | âœ… Full | Describe consumer groups |
| ListGroups | v0-v4 | âœ… Full | List all groups |
| SaslHandshake | v0-v1 | âœ… Full | SASL authentication |
| SaslAuthenticate | v0-v2 | âœ… Full | SASL auth exchange |
| CreatePartitions | v0-v3 | âœ… Full | Add partitions to topics |
| InitProducerId | v0-v4 | âœ… Full | Initialize transactional producer |
| AddPartitionsToTxn | v0-v3 | âœ… Full | Add partitions to transaction |
| AddOffsetsToTxn | v0-v3 | âœ… Full | Add consumer offsets to transaction |
| EndTxn | v0-v3 | âœ… Full | Commit or abort transaction |
| TxnOffsetCommit | v0-v3 | âœ… Full | Commit offsets within transaction |

### Tested Clients

- âœ… **kafka-python** - Python client (full compatibility)
- âœ… **confluent-kafka** - High-performance C-based client
- âœ… **KSQLDB** - Full support including transactional operations
- âœ… **Apache Flink** - Stream processing integration

## ğŸ› ï¸ Development

### Prerequisites

- Rust 1.75+
- Docker & Docker Compose (for testing)
- Python 3.8+ with kafka-python (for client testing)

### Building

```bash
# Build all components
cargo build --release

# Run tests (unit and bin tests only)
cargo test --workspace --lib --bins

# Run integration tests (requires setup)
cargo test --test integration

# Run benchmarks
cargo bench
```

### Project Structure

```
chronik-stream/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ chronik-server/      # Main server binary (unified)
â”‚   â”œâ”€â”€ chronik-protocol/    # Kafka wire protocol implementation
â”‚   â”œâ”€â”€ chronik-storage/     # Storage abstraction layer
â”‚   â”œâ”€â”€ chronik-search/      # Search engine integration (Tantivy)
â”‚   â”œâ”€â”€ chronik-query/       # Query processing
â”‚   â”œâ”€â”€ chronik-common/      # Shared utilities
â”‚   â”œâ”€â”€ chronik-auth/        # Authentication & authorization
â”‚   â”œâ”€â”€ chronik-monitoring/  # Metrics & observability
â”‚   â”œâ”€â”€ chronik-config/      # Configuration management
â”‚   â”œâ”€â”€ chronik-backup/      # Backup functionality
â”‚   â”œâ”€â”€ chronik-bench/       # Performance benchmarking tool
â”‚   â”œâ”€â”€ chronik-wal/         # Write-Ahead Log & metadata store
â”‚   â”œâ”€â”€ chronik-raft/        # Raft consensus implementation
â”‚   â””â”€â”€ chronik-raft-bridge/ # Raft integration bridge
â”œâ”€â”€ tests/                   # Integration tests
â”œâ”€â”€ Dockerfile              # Multi-arch Docker build
â”œâ”€â”€ docker-compose.yml      # Local development setup
â””â”€â”€ .github/workflows/      # CI/CD pipelines
```

## âš¡ Performance

Chronik Stream delivers exceptional performance across all deployment modes (128 concurrency, 256 byte messages):

### Benchmarks

| Mode | Configuration | Throughput | p99 Latency |
|------|---------------|------------|-------------|
| **Standalone** | acks=1 | **309K msg/s** | 0.59ms |
| **Standalone** | acks=all | **348K msg/s** | 0.56ms |
| **Cluster (3 nodes)** | acks=1 | **188K msg/s** | 2.81ms |
| **Cluster (3 nodes)** | acks=all | **166K msg/s** | 1.80ms |

#### Searchable Topics Impact

| Configuration | Non-Searchable | Searchable | Overhead |
|--------------|---------------:|-----------:|---------:|
| Standalone | 198K msg/s | 192K msg/s | 3% |
| Cluster (3 nodes) | 183K msg/s | 123K msg/s | 33% |

### Key Performance Features

- **High Throughput**: Up to 348K messages/second standalone, 188K cluster
- **Low Latency**: Sub-millisecond p99 latency standalone, sub-3ms cluster
- **Efficient Memory**: Zero-copy networking with minimal allocations
- **Recovery**: 100% message recovery with zero duplicates
- **Search**: Only 3% overhead for real-time Tantivy indexing (standalone)
- **Compression**: Snappy, LZ4, Zstd for efficient storage

### WAL Performance
- **Write Throughput**: 300K+ msgs/sec with GroupCommitWal
- **Recovery Speed**: Full recovery in seconds even for large datasets
- **Zero Data Loss**: All acks modes (0, 1, -1) guaranteed durable
- **Group Commit**: PostgreSQL-style batched fsync reduces I/O overhead

See [BASELINE_PERFORMANCE.md](BASELINE_PERFORMANCE.md) for detailed benchmark methodology and results.

## ğŸ”’ Security

### SASL Authentication

Chronik Stream supports SASL authentication with the following mechanisms:
- **PLAIN** - Username/password authentication
- **SCRAM-SHA-256** - Challenge-response authentication
- **SCRAM-SHA-512** - Challenge-response authentication (stronger)

**Default users** (for development/testing):
| Username | Password |
|----------|----------|
| admin | admin123 |
| user | user123 |
| kafka | kafka-secret |

```python
# Python example with SASL/PLAIN
from kafka import KafkaProducer

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    security_protocol='SASL_PLAINTEXT',
    sasl_mechanism='PLAIN',
    sasl_plain_username='admin',
    sasl_plain_password='admin123'
)
```

### Additional Security Features

- **TLS/SSL**: End-to-end encryption (infrastructure in `chronik-auth` crate)
- **ACLs**: Topic and consumer group access control framework
- **Admin API**: Secured with API key authentication (cluster management)

## ğŸ“Š Monitoring

### Prometheus Metrics

```bash
# Expose metrics endpoint
chronik --metrics-port 9093

# Key metrics:
- chronik_messages_received_total
- chronik_messages_stored_total
- chronik_produce_latency_seconds
- chronik_fetch_latency_seconds
- chronik_storage_usage_bytes
- chronik_active_connections
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

Apache License 2.0. See [LICENSE](LICENSE) for details.

## ğŸ“š Documentation

### Getting Started
- [CHANGELOG.md](CHANGELOG.md) - Detailed release history
- [docs/RUNNING_A_CLUSTER.md](docs/RUNNING_A_CLUSTER.md) - **Complete cluster setup guide (v2.2.0+)**
- [docs/SEARCHABLE_TOPICS.md](docs/SEARCHABLE_TOPICS.md) - **Searchable topics with real-time indexing (v2.2.16+)**
- [docs/KSQL_INTEGRATION_GUIDE.md](docs/KSQL_INTEGRATION_GUIDE.md) - KSQL setup and usage

### v2.2.8 Release (Critical Fixes)
- [docs/WATERMARK_IDEMPOTENCE_FIX_v2.2.7.md](docs/WATERMARK_IDEMPOTENCE_FIX_v2.2.7.md) - Watermark idempotence fix details
- [docs/WATERMARK_OVERWRITE_BUG_ROOT_CAUSE.md](docs/WATERMARK_OVERWRITE_BUG_ROOT_CAUSE.md) - Root cause analysis
- [docs/100_PERCENT_CONSUMPTION_INVESTIGATION.md](docs/100_PERCENT_CONSUMPTION_INVESTIGATION.md) - **100% consumption guide**
- [docs/WATERMARK_REPLICATION_TEST_RESULTS_v2.2.7.2.md](docs/WATERMARK_REPLICATION_TEST_RESULTS_v2.2.7.2.md) - Test results and findings

### Operations & Performance
- [BASELINE_PERFORMANCE.md](BASELINE_PERFORMANCE.md) - **Performance benchmarks (standalone vs cluster, searchable vs non-searchable)**
- [docs/WAL_AUTO_TUNING.md](docs/WAL_AUTO_TUNING.md) - WAL performance auto-tuning guide
- [docs/DISASTER_RECOVERY.md](docs/DISASTER_RECOVERY.md) - Disaster recovery and backup strategies
- [docs/ADMIN_API_SECURITY.md](docs/ADMIN_API_SECURITY.md) - Admin API security configuration

### Cluster Management (v2.2.0+)
- [docs/TESTING_NODE_REMOVAL.md](docs/TESTING_NODE_REMOVAL.md) - Testing node addition/removal
- [docs/PRIORITY4_COMPLETE.md](docs/PRIORITY4_COMPLETE.md) - Node removal implementation details

### Development
- [CLAUDE.md](CLAUDE.md) - Development guide for AI assistants
- [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) - System architecture and design
- [docs/BUILD_INSTRUCTIONS.md](docs/BUILD_INSTRUCTIONS.md) - Build and development setup
