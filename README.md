# Chronik Stream

[![Build Status](https://github.com/lspecian/chronik-stream/workflows/CI/badge.svg)](https://github.com/lspecian/chronik-stream/actions)
[![Release](https://img.shields.io/github/v/release/lspecian/chronik-stream)](https://github.com/lspecian/chronik-stream/releases)
[![Docker Image](https://img.shields.io/badge/docker-ghcr.io-blue)](https://github.com/lspecian/chronik-stream/pkgs/container/chronik-stream)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/rust-1.75%2B-orange.svg)](https://www.rust-lang.org)

A high-performance streaming platform built in Rust that implements core Kafka wire protocol functionality with comprehensive Write-Ahead Log (WAL) durability and automatic recovery.

See [CHANGELOG.md](CHANGELOG.md) for release history and latest updates.

## ğŸš€ Features

- **Kafka Wire Protocol**: Full Kafka wire protocol with consumer group and transactional support
- **Full Compression Support**: All Kafka compression codecs (Gzip, Snappy, LZ4, Zstd) - see [COMPRESSION_SUPPORT.md](COMPRESSION_SUPPORT.md)
- **WAL-based Metadata**: ChronikMetaLog provides event-sourced metadata persistence
- **GroupCommitWal**: PostgreSQL-style group commit with per-partition background workers and batched fsync
- **Zero Message Loss**: WAL ensures durability for all acks modes (0, 1, -1) even during unexpected shutdowns
- **Automatic Recovery**: WAL records are automatically replayed on startup to restore state with 100% accuracy
- **Real Client Testing**: Tested with kafka-python, confluent-kafka, KSQL, and Apache Flink
- **Stress Tested**: Verified at scale with 50K+ messages, zero duplicates, 39K+ msgs/sec throughput
- **Transactional APIs**: Full support for Kafka transactions (InitProducerId, AddPartitionsToTxn, EndTxn)
- **High Performance**: Async architecture with zero-copy networking optimizations
- **Multi-Architecture**: Native support for x86_64 and ARM64 (Apple Silicon, AWS Graviton)
- **Container Ready**: Docker deployment with proper network configuration
- **Simplified Operations**: Single-process architecture reduces operational complexity

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka Client  â”‚â”€â”€â”€â”€â–¶â”‚            Chronik Server               â”‚
â”‚  (Any Language) â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚ Kafka Proto â”‚  â”‚  ChronikMetaLog â”‚  â”‚
                        â”‚  â”‚ Handler     â”‚  â”‚  (WAL Metadata) â”‚  â”‚
                        â”‚  â”‚ (Port 9092) â”‚  â”‚                 â”‚  â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                        â”‚  â”‚   Search    â”‚  â”‚  Storage Mgr    â”‚  â”‚
                        â”‚  â”‚  (Tantivy)  â”‚  â”‚                 â”‚  â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚   Object Storage    â”‚
                                â”‚  (S3/GCS/Local)     â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš¡ Quick Start

### Using Docker (Recommended)

```bash
# Quick start - single command
docker run -d -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  ghcr.io/lspecian/chronik-stream:latest

# With persistent storage and custom configuration
docker run -d --name chronik \
  -p 9092:9092 \
  -v chronik-data:/data \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e RUST_LOG=info \
  ghcr.io/lspecian/chronik-stream:latest

# Using docker-compose
curl -O https://raw.githubusercontent.com/lspecian/chronik-stream/main/docker-compose.yml
docker-compose up -d
```

### With S3/MinIO Object Storage

```bash
# MinIO for development
docker run -d --name chronik \
  -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e OBJECT_STORE_BACKEND=s3 \
  -e S3_ENDPOINT=http://minio:9000 \
  -e S3_BUCKET=chronik-storage \
  -e S3_ACCESS_KEY=minioadmin \
  -e S3_SECRET_KEY=minioadmin \
  -e S3_PATH_STYLE=true \
  ghcr.io/lspecian/chronik-stream:latest

# AWS S3 for production (uses IAM role)
docker run -d --name chronik \
  -p 9092:9092 \
  -e CHRONIK_ADVERTISED_ADDR=localhost \
  -e OBJECT_STORE_BACKEND=s3 \
  -e S3_REGION=us-west-2 \
  -e S3_BUCKET=chronik-prod-archives \
  ghcr.io/lspecian/chronik-stream:latest
```

### âš ï¸ Critical Docker Configuration

**IMPORTANT**: When running Chronik Stream in Docker or binding to `0.0.0.0`, you **MUST** set `CHRONIK_ADVERTISED_ADDR`:

```yaml
# docker-compose.yml example
services:
  chronik-stream:
    image: ghcr.io/lspecian/chronik-stream:latest
    ports:
      - "9092:9092"
    environment:
      CHRONIK_BIND_ADDR: "0.0.0.0"  # Just host, no port
      CHRONIK_ADVERTISED_ADDR: "chronik-stream"  # REQUIRED - use container name for Docker networks
      # or "localhost" for host access, or your public hostname/IP for remote access
```

Without `CHRONIK_ADVERTISED_ADDR`, clients will receive `0.0.0.0:9092` in metadata responses and fail to connect.

### Test with Kafka Client

```python
# Python example
from kafka import KafkaProducer, KafkaConsumer

# Producer
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    api_version=(0, 10, 0)  # Important: specify version
)
producer.send('test-topic', b'Hello Chronik!')
producer.flush()

# Consumer
consumer = KafkaConsumer(
    'test-topic',
    bootstrap_servers='localhost:9092',
    api_version=(0, 10, 0),
    auto_offset_reset='earliest'
)
for message in consumer:
    print(f"Received: {message.value}")
```

### Using Binary

```bash
# Download latest release (Linux x86_64)
curl -L https://github.com/lspecian/chronik-stream/releases/latest/download/chronik-server-linux-amd64.tar.gz -o chronik-server.tar.gz
tar xzf chronik-server.tar.gz
./chronik-server --advertised-addr localhost standalone

# macOS (Apple Silicon)
curl -L https://github.com/lspecian/chronik-stream/releases/latest/download/chronik-server-darwin-arm64.tar.gz -o chronik-server.tar.gz
tar xzf chronik-server.tar.gz
./chronik-server --advertised-addr localhost standalone
```

### Building from Source

```bash
# Clone repository
git clone https://github.com/lspecian/chronik-stream.git
cd chronik-stream

# Build release binary
cargo build --release --bin chronik-server

# Run
./target/release/chronik-server standalone
```

## ğŸŒŸ KSQL Integration

Chronik Stream provides **full compatibility** with KSQLDB (Confluent's SQL engine for Kafka) including transactional support. Simply point KSQLDB at Chronik's Kafka endpoint:

```properties
# ksql-server.properties
bootstrap.servers=localhost:9092
ksql.service.id=ksql_service_1
```

For detailed KSQL setup and usage examples, see [docs/KSQL_INTEGRATION_GUIDE.md](docs/KSQL_INTEGRATION_GUIDE.md).

## ğŸ¯ Operational Modes

The unified `chronik-server` binary supports multiple operational modes:

### Standalone Mode (Default)
Single-node Kafka-compatible server, perfect for development and small deployments:
```bash
chronik-server standalone
# or just
chronik-server
```

### All Mode
Run all components (Kafka protocol, search, backup) in a single process:
```bash
chronik-server all
```

### Configuration Options
```bash
chronik-server [OPTIONS] [COMMAND]

Options:
  -p, --kafka-port <PORT>      Kafka protocol port (default: 9092)
  -a, --admin-port <PORT>      Admin API port (default: 3000)
  -d, --data-dir <PATH>        Data directory (default: ./data)
  -b, --bind-addr <ADDR>       Bind address (default: 0.0.0.0)
  --advertised-addr <ADDR>     Address advertised to clients (REQUIRED for Docker/remote access)
  --advertised-port <PORT>     Port advertised to clients (default: kafka port)
  --file-metadata              Use file-based metadata store instead of WAL-based (legacy mode)
  --enable-search              Enable search functionality
  --enable-backup              Enable backup functionality

Environment Variables:
  CHRONIK_KAFKA_PORT           Kafka protocol port
  CHRONIK_BIND_ADDR            Server bind address (just host, no port)
  CHRONIK_ADVERTISED_ADDR      Address advertised to clients (CRITICAL for Docker)
  CHRONIK_ADVERTISED_PORT      Port advertised to clients
  CHRONIK_DATA_DIR             Data directory path
  CHRONIK_FILE_METADATA        Set to "true" to use legacy file-based metadata store
  CHRONIK_WAL_PROFILE          WAL performance profile: low/medium/high/ultra (auto-detects if not set)
  CHRONIK_PRODUCE_PROFILE      Producer flush profile: low-latency/balanced/high-throughput (default: balanced)
  RUST_LOG                     Log level (error, warn, info, debug, trace)

Object Store (Tier 3 - Tantivy Archives):
  OBJECT_STORE_BACKEND         Backend type: s3/gcs/azure/local (default: local)

  S3 Configuration:
    S3_ENDPOINT                S3-compatible endpoint (for MinIO, Wasabi, etc.)
    S3_REGION                  AWS region (default: us-east-1)
    S3_BUCKET                  Bucket name (default: chronik-storage)
    S3_ACCESS_KEY              Access key ID (optional, uses IAM if not set)
    S3_SECRET_KEY              Secret access key (optional)
    S3_PATH_STYLE              Use path-style URLs (default: true, required for MinIO)
    S3_DISABLE_SSL             Disable SSL (default: false)
    S3_PREFIX                  Key prefix for all objects (optional)

  GCS Configuration:
    GCS_BUCKET                 GCS bucket name (default: chronik-storage)
    GCS_PROJECT_ID             GCP project ID (optional)
    GCS_PREFIX                 Key prefix for all objects (optional)

  Azure Configuration:
    AZURE_ACCOUNT_NAME         Storage account name (required)
    AZURE_CONTAINER            Container name (default: chronik-storage)
    AZURE_USE_EMULATOR         Use Azurite emulator (default: false)
```

## âš¡ Performance Tuning

Chronik Stream provides two layers of performance tuning for different workloads:

### Producer Flush Profiles

Control when buffered messages become visible to consumers:

**Low-Latency** - Real-time applications
```bash
CHRONIK_PRODUCE_PROFILE=low-latency ./chronik-server ...
```
- Settings: 1 batch / 10ms flush / 16MB buffer
- Target: < 20ms p99 latency
- Use for: Real-time analytics, instant messaging, live dashboards

**Balanced** - General-purpose (default)
```bash
./chronik-server ...  # No env var needed
```
- Settings: 10 batches / 100ms flush / 32MB buffer
- Target: 100-150ms p99 latency
- Use for: General streaming, typical microservices

**High-Throughput** - Bulk ingestion
```bash
CHRONIK_PRODUCE_PROFILE=high-throughput ./chronik-server ...
```
- Settings: 100 batches / 500ms flush / 128MB buffer
- Target: < 500ms p99 latency
- Use for: Log aggregation, data pipelines, ETL, batch processing

### WAL Performance Profiles

The Write-Ahead Log automatically detects system resources (CPU, memory, Docker/K8s limits) and selects an appropriate profile. Override with:

```bash
CHRONIK_WAL_PROFILE=low        # Containers, small VMs (â‰¤1 CPU, <512MB)
CHRONIK_WAL_PROFILE=medium     # Typical servers (2-4 CPUs, 512MB-4GB)
CHRONIK_WAL_PROFILE=high       # Dedicated servers (4-16 CPUs, 4-16GB)
CHRONIK_WAL_PROFILE=ultra      # Maximum throughput (16+ CPUs, 16GB+)
```

### Benchmarking

Test performance profiles with your workload:
```bash
cargo build --release
python3 tests/test_profiles_quick.py  # Quick 10K message test
python3 tests/test_produce_profiles.py  # Comprehensive benchmark
```

## ğŸ“¦ Docker Images

All images support both **linux/amd64** and **linux/arm64** architectures:

| Image | Tags | Description |
|-------|------|-------------|
| `ghcr.io/lspecian/chronik-stream` | `latest`, `1.3`, `v1.3.46` | Chronik server with full KSQL support |

### Supported Platforms

- âœ… **Linux x86_64** (amd64)
- âœ… **Linux ARM64** (aarch64) - AWS Graviton, Raspberry Pi 4+
- âœ… **macOS x86_64** (Intel)
- âœ… **macOS ARM64** (Apple Silicon M1/M2/M3)

## âœ… Kafka Compatibility

### Supported Kafka APIs (24 total)

| API | Version | Status | Description |
|-----|---------|--------|-------------|
| Produce | v0-v9 | âœ… Full | Send messages to topics |
| Fetch | v0-v13 | âœ… Full | Retrieve messages from topics |
| ListOffsets | v0-v7 | âœ… Full | Query partition offsets |
| Metadata | v0-v12 | âœ… Full | Get cluster metadata |
| OffsetCommit | v0-v8 | âœ… Full | Commit consumer offsets |
| OffsetFetch | v0-v8 | âœ… Full | Retrieve consumer offsets |
| FindCoordinator | v0-v4 | âœ… Full | Find group coordinator |
| JoinGroup | v0-v9 | âœ… Full | Join consumer group |
| Heartbeat | v0-v4 | âœ… Full | Consumer heartbeat |
| LeaveGroup | v0-v5 | âœ… Full | Leave consumer group |
| SyncGroup | v0-v5 | âœ… Full | Sync group assignments |
| ApiVersions | v0-v3 | âœ… Full | Negotiate API versions |
| CreateTopics | v0-v7 | âœ… Full | Create new topics |
| DeleteTopics | v0-v6 | âœ… Full | Delete topics |
| DescribeGroups | v0-v5 | âœ… Full | Describe consumer groups |
| ListGroups | v0-v4 | âœ… Full | List all groups |
| SaslHandshake | v0-v1 | âœ… Full | SASL authentication |
| SaslAuthenticate | v0-v2 | âœ… Full | SASL auth exchange |
| CreatePartitions | v0-v3 | âœ… Full | Add partitions to topics |
| InitProducerId | v0-v4 | âœ… Full | Initialize transactional producer |
| AddPartitionsToTxn | v0-v3 | âœ… Full | Add partitions to transaction |
| AddOffsetsToTxn | v0-v3 | âœ… Full | Add consumer offsets to transaction |
| EndTxn | v0-v3 | âœ… Full | Commit or abort transaction |
| TxnOffsetCommit | v0-v3 | âœ… Full | Commit offsets within transaction |

### Tested Clients

- âœ… **kafka-python** - Python client (full compatibility)
- âœ… **confluent-kafka** - High-performance C-based client
- âœ… **KSQLDB** - Full support including transactional operations
- âœ… **Apache Flink** - Stream processing integration

## ğŸ› ï¸ Development

### Prerequisites

- Rust 1.75+
- Docker & Docker Compose (for testing)
- Python 3.8+ with kafka-python (for client testing)

### Building

```bash
# Build all components
cargo build --release

# Run tests (unit and bin tests only)
cargo test --workspace --lib --bins

# Run integration tests (requires setup)
cargo test --test integration

# Run benchmarks
cargo bench
```

### Project Structure

```
chronik-stream/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ chronik-server/      # Main server binary (unified)
â”‚   â”œâ”€â”€ chronik-protocol/    # Kafka wire protocol implementation
â”‚   â”œâ”€â”€ chronik-storage/     # Storage abstraction layer
â”‚   â”œâ”€â”€ chronik-search/      # Search engine integration
â”‚   â”œâ”€â”€ chronik-query/       # Query processing
â”‚   â”œâ”€â”€ chronik-common/      # Shared utilities
â”‚   â”œâ”€â”€ chronik-auth/        # Authentication & authorization
â”‚   â”œâ”€â”€ chronik-monitoring/  # Metrics & observability
â”‚   â”œâ”€â”€ chronik-config/      # Configuration management
â”‚   â”œâ”€â”€ chronik-backup/      # Backup functionality
â”‚   â”œâ”€â”€ chronik-benchmarks/  # Performance benchmarks
â”‚   â”œâ”€â”€ chronik-cli/         # Command line interface
â”‚   â””â”€â”€ chronik-wal/         # Write-Ahead Log & metadata store
â”œâ”€â”€ tests/                   # Integration tests
â”œâ”€â”€ Dockerfile              # Multi-arch Docker build
â”œâ”€â”€ docker-compose.yml      # Local development setup
â””â”€â”€ .github/workflows/      # CI/CD pipelines
```

## âš¡ Performance

Chronik Stream is optimized for production workloads:

- **CPU Usage**: 0% idle (efficient resource utilization)
- **Memory**: Efficient memory usage with zero-copy networking
- **Latency**: < 10ms p99 produce latency with acks=1
- **Throughput**: 39K+ messages/second (tested at 50K message scale)
- **Recovery**: 100% message recovery with zero duplicates
- **Search**: Sub-second full-text search with Tantivy
- **Storage**: Efficient compression (Snappy, LZ4, Zstd)

### WAL Performance
- **Write Throughput**: 39,200 msgs/sec for acks=1 (50K message test)
- **Recovery Speed**: Full recovery in seconds even for large datasets
- **Zero Data Loss**: All acks modes (0, 1, -1) guaranteed durable
- **Group Commit**: PostgreSQL-style batched fsync reduces I/O overhead

## ğŸ”’ Security

- **SASL Authentication**: PLAIN, SCRAM-SHA-256/512
- **TLS/SSL**: End-to-end encryption
- **ACLs**: Topic and consumer group access control
- **Audit Logging**: Track all administrative actions

## ğŸ“Š Monitoring

### Prometheus Metrics

```bash
# Expose metrics endpoint
chronik --metrics-port 9093

# Key metrics:
- chronik_messages_received_total
- chronik_messages_stored_total
- chronik_produce_latency_seconds
- chronik_fetch_latency_seconds
- chronik_storage_usage_bytes
- chronik_active_connections
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

Apache License 2.0. See [LICENSE](LICENSE) for details.

## ğŸ“š Documentation

- [CHANGELOG.md](CHANGELOG.md) - Detailed release history
- [CLAUDE.md](CLAUDE.md) - Development guide for AI assistants
- [docs/KSQL_INTEGRATION_GUIDE.md](docs/KSQL_INTEGRATION_GUIDE.md) - KSQL setup and usage
- [docs/WAL_AUTO_TUNING.md](docs/WAL_AUTO_TUNING.md) - WAL performance auto-tuning guide
