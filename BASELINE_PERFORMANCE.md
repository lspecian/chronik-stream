# Baseline Performance Results

## Test Date: 2025-11-25

### Test Configuration
- **Server**: Standalone single-node
- **Concurrency**: 128 producers
- **Message Size**: 256 bytes
- **Duration**: 30 seconds
- **Acks**: 1 (wait for leader)
- **Compression**: None
- **Command**: `./target/release/chronik-bench -b localhost:9092 -c 128 -s 256 -d 30s --acks 1 --topic standalone-perf-test --create-topic`

### Performance Results

#### Clean Build (No Diagnostic Logging) ‚úÖ

| Metric | Value |
|--------|-------|
| **Average Message Rate** | **333,321 msg/s** |
| **Peak Message Rate** | **348,258 msg/s** |
| **Total Messages** | 10,000,168 |
| **Bandwidth** | 81.38 MB/s |
| **Success Rate** | 100% (0 failures) |
| **Duration** | 30.00s |

**Latency Distribution:**

| Percentile | Latency (Œºs) | Latency (ms) |
|------------|--------------|--------------|
| p50 | 348 | 0.35 |
| p90 | 479 | 0.48 |
| p95 | 539 | 0.54 |
| p99 | 808 | 0.81 |
| p99.9 | 3,955 | 3.96 |
| max | 6,271 | 6.27 |

#### With Diagnostic Logging ‚ùå

| Metric | Value |
|--------|-------|
| **Average Message Rate** | 62,602 msg/s |
| **Peak Message Rate** | 77,300 msg/s |
| **Total Messages** | 2,191,185 |
| **Bandwidth** | 15.28 MB/s |
| **Success Rate** | 100% (0 failures) |
| **Duration** | 35.00s (including warmup) |

**Latency Distribution:**

| Percentile | Latency (Œºs) | Latency (ms) |
|------------|--------------|--------------|
| p50 | 1,694 | 1.69 |
| p90 | 2,295 | 2.29 |
| p95 | 2,525 | 2.52 |
| p99 | 3,969 | 3.97 |
| p99.9 | 4,259 | 4.26 |
| max | 6,835 | 6.83 |

### üîç Root Cause Identified: Diagnostic Logging

**Performance Impact of Diagnostic Logging:**

| Metric | Clean Build | With Logging | Impact |
|--------|-------------|--------------|--------|
| Message Rate | 333,321 msg/s | 62,602 msg/s | **5.3x slower** |
| p99 Latency | 0.81 ms | 3.97 ms | **4.9x higher** |
| p50 Latency | 0.35 ms | 1.69 ms | **4.8x higher** |
| Throughput | 81.38 MB/s | 15.28 MB/s | **5.3x slower** |

**Diagnostic Logging Changes (git stash):**
1. `integrated_server.rs`: Added connection handler debug logs
2. `produce_handler.rs`: Added WAL path diagnostic logs (multiple `info!()` calls in hot path)
3. `produce_handler.rs`: Increased pipelined_pool capacity (1000 ‚Üí 10000) - not the cause

**Conclusion**: The `info!()` logging calls in the hot path (produce handler WAL writes) caused a **5.3x performance regression**. The clean build achieves the expected **333k msg/s baseline** mentioned in [CLUSTER_RDKAFKA_HANG_ANALYSIS.md](CLUSTER_RDKAFKA_HANG_ANALYSIS.md).

### ‚úÖ Investigation Complete

1. ‚úÖ Document current baseline (this file)
2. ‚úÖ Check git status for modified files
3. ‚úÖ Stash changes and rebuild from clean state
4. ‚úÖ Retest with same parameters to validate 300k msg/s baseline
5. ‚úÖ Identified regression cause: Diagnostic logging in hot path

**Recommendation**: Remove diagnostic logging from hot paths. Use conditional compilation (`#[cfg(feature = "trace-logging")]`) for performance-critical diagnostic logging, or use trace-level logs that can be disabled at runtime.

---

**Git Status**: Clean (diagnostic changes stashed)
**Stash**: `git stash list` shows "Diagnostic logging and capacity increase - testing performance baseline"
**Build Version**: v2.2.9
**Test System**: Ubuntu Linux 6.11.0-28-generic
